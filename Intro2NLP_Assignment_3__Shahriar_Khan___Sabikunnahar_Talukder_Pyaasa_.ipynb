{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cMSAkX0MHBif",
   "metadata": {
    "id": "cMSAkX0MHBif"
   },
   "source": [
    "Submitted by: Shahriar Khan 50311961 | Sabikunnahar Talukder Pyaasa 50325061"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339230a-ed92-45af-a07b-eea11572b1d8",
   "metadata": {
    "id": "c339230a-ed92-45af-a07b-eea11572b1d8"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 3\n",
    "\n",
    "In this exercise we'll practice training RNN & LSTM models as well as fine-tuning LLMs to predict one or more labels for a given text using Hugging Face and PyTorch.\n",
    "\n",
    "- You can use any Python package you need.\n",
    "- Please comment your code\n",
    "- Submissions are due Tuesdays at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 3 (Deadline: 10.12.2024, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately: \"Assignment_3_\\<Your_Name\\>.ipynb\" and submit only the Jupyter Notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41aaa1",
   "metadata": {},
   "source": [
    "### Grade Summary:\n",
    "\n",
    "**Task 1:**   12/15\n",
    "\n",
    "**Task 2:**   2/2\n",
    "\n",
    "**Task 3:**   2/3\n",
    "\n",
    "**Task 4:**   15/15\n",
    "\n",
    "### **Total:**  31/35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c845daa-4566-4abe-9914-2f9b35544c31",
   "metadata": {
    "id": "6c845daa-4566-4abe-9914-2f9b35544c31"
   },
   "source": [
    "### Task 1 (15 points)\n",
    "\n",
    "In this task you will implement text generation in torch with a multi-layer RNN and multi-layer LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b41569-0edf-4653-a059-17e54b510fb2",
   "metadata": {
    "id": "14b41569-0edf-4653-a059-17e54b510fb2"
   },
   "source": [
    "a) Implement the missing methods of the dataset class to\n",
    "1. load the dataset from the file `reddit-cleanjokes.csv` and split it into words [**(dataset link)**](https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv)\n",
    "2. get a list of the unique words\n",
    "3. implement the `__getitem__` method to iterate through the dataset. Hint: use `torch.tensor` to turn a list into a tensor.\n",
    "\n",
    "Then instantiate the dataset with `sequence_length=4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6dac7-61cb-4373-b6a5-37e12856c78b",
   "metadata": {
    "id": "80f6dac7-61cb-4373-b6a5-37e12856c78b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "# hint: use these methods\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "    ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all words in the dataset.\n",
    "        Make sure to strip punctuation and lowercase the words\"\"\"\n",
    "        # Load the dataset from the CSV file\n",
    "        data = read_csv(\"reddit-cleanjokes.csv\")  # Update with the correct path\n",
    "        text = \" \".join(data[\"Joke\"])\n",
    "\n",
    "        # Clean the text: remove punctuation and lowercase\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        return words\n",
    "\n",
    "    def get_uniq_words(self) -> List[str]:\n",
    "        \"\"\"Returns a list, containing each unique word in the dataset once\"\"\"\n",
    "        return list(Counter(self.words).keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of `self.sequence_length` length word spans in the dataset\"\"\"\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index) -> (Tensor, Tensor):\n",
    "        \"\"\"Returns a tuple of two torch.Tensors:\n",
    "        an input sequence for the RNN/LSTM model and a target sequence.\n",
    "        The tensors should be 1D and have length equal to self.sequence_length.\n",
    "        Remember that the target should be shifted with respect to the input.\"\"\"\n",
    "        # Input sequence\n",
    "        input_seq = self.words_indexes[index:index + self.sequence_length]\n",
    "\n",
    "        # Target sequence (shifted by one)\n",
    "        target_seq = self.words_indexes[index + 1:index + 1 + self.sequence_length]\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e2030-39ab-4e7d-874e-3bcb1ad35272",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e67e2030-39ab-4e7d-874e-3bcb1ad35272",
    "outputId": "5512350a-3ba7-4b8c-b372-39d96157fdf2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'bartender',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'jumper',\n",
       " 'cables',\n",
       " 'you',\n",
       " 'better',\n",
       " 'not',\n",
       " 'try',\n",
       " 'to',\n",
       " 'start',\n",
       " 'anything',\n",
       " 'dont',\n",
       " 'you',\n",
       " 'hate',\n",
       " 'jokes',\n",
       " 'about',\n",
       " 'german',\n",
       " 'sausage',\n",
       " 'theyre',\n",
       " 'the',\n",
       " 'wurst',\n",
       " 'two',\n",
       " 'artists',\n",
       " 'had',\n",
       " 'an',\n",
       " 'art',\n",
       " 'contest',\n",
       " 'it',\n",
       " 'ended',\n",
       " 'in',\n",
       " 'a',\n",
       " 'draw',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'chicken',\n",
       " 'cross',\n",
       " 'the',\n",
       " 'playground',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'the',\n",
       " 'other',\n",
       " 'slide',\n",
       " 'what',\n",
       " 'gun',\n",
       " 'do',\n",
       " 'you',\n",
       " 'use',\n",
       " 'to',\n",
       " 'hunt',\n",
       " 'a',\n",
       " 'moose',\n",
       " 'a',\n",
       " 'moosecut',\n",
       " 'if',\n",
       " 'life',\n",
       " 'gives',\n",
       " 'you',\n",
       " 'melons',\n",
       " 'you',\n",
       " 'might',\n",
       " 'have',\n",
       " 'dyslexia',\n",
       " 'broken',\n",
       " 'pencils',\n",
       " 'are',\n",
       " 'pointless',\n",
       " 'what',\n",
       " 'did',\n",
       " 'one',\n",
       " 'snowman',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'other',\n",
       " 'snowman',\n",
       " 'do',\n",
       " 'you',\n",
       " 'smell',\n",
       " 'carrots',\n",
       " 'how',\n",
       " 'many',\n",
       " 'hipsters',\n",
       " 'does',\n",
       " 'it',\n",
       " 'take',\n",
       " 'to',\n",
       " 'change',\n",
       " 'a',\n",
       " 'lightbulb',\n",
       " 'its',\n",
       " 'a',\n",
       " 'really',\n",
       " 'obscure',\n",
       " 'number',\n",
       " 'youve',\n",
       " 'probably',\n",
       " 'never',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'it',\n",
       " 'where',\n",
       " 'do',\n",
       " 'sick',\n",
       " 'boats',\n",
       " 'go',\n",
       " 'the',\n",
       " 'dock',\n",
       " 'i',\n",
       " 'like',\n",
       " 'my',\n",
       " 'slaves',\n",
       " 'like',\n",
       " 'i',\n",
       " 'like',\n",
       " 'my',\n",
       " 'coffee',\n",
       " 'free',\n",
       " 'my',\n",
       " 'girlfriend',\n",
       " 'told',\n",
       " 'me',\n",
       " 'she',\n",
       " 'was',\n",
       " 'leaving',\n",
       " 'me',\n",
       " 'because',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'pretending',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'transformer',\n",
       " 'i',\n",
       " 'said',\n",
       " 'no',\n",
       " 'wait',\n",
       " 'i',\n",
       " 'can',\n",
       " 'change',\n",
       " 'old',\n",
       " 'chinese',\n",
       " 'proverb',\n",
       " 'man',\n",
       " 'who',\n",
       " 'not',\n",
       " 'shower',\n",
       " 'in',\n",
       " '7',\n",
       " 'days',\n",
       " 'makes',\n",
       " 'one',\n",
       " 'reek',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'of',\n",
       " 'a',\n",
       " 'brownie',\n",
       " 'factory',\n",
       " 'say',\n",
       " 'when',\n",
       " 'his',\n",
       " 'factory',\n",
       " 'caught',\n",
       " 'fire',\n",
       " 'im',\n",
       " 'getting',\n",
       " 'the',\n",
       " 'fudge',\n",
       " 'outta',\n",
       " 'here',\n",
       " 'what',\n",
       " 'form',\n",
       " 'of',\n",
       " 'radiation',\n",
       " 'bakes',\n",
       " 'you',\n",
       " 'cookies',\n",
       " 'a',\n",
       " 'gramma',\n",
       " 'ray',\n",
       " 'bee',\n",
       " 'jokes',\n",
       " 'courtesy',\n",
       " 'of',\n",
       " 'my',\n",
       " 'niece',\n",
       " 'age',\n",
       " '8',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'bee',\n",
       " 'use',\n",
       " 'to',\n",
       " 'dry',\n",
       " 'off',\n",
       " 'after',\n",
       " 'swimming',\n",
       " 'a',\n",
       " 'beech',\n",
       " 'towel',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'bee',\n",
       " 'use',\n",
       " 'to',\n",
       " 'get',\n",
       " 'out',\n",
       " 'the',\n",
       " 'tangles',\n",
       " 'a',\n",
       " 'honeycomb',\n",
       " 'whats',\n",
       " 'the',\n",
       " 'loudest',\n",
       " 'economic',\n",
       " 'system',\n",
       " 'capitalism',\n",
       " 'i',\n",
       " 'went',\n",
       " 'for',\n",
       " 'a',\n",
       " 'job',\n",
       " 'interview',\n",
       " 'today',\n",
       " 'the',\n",
       " 'interviewer',\n",
       " 'said',\n",
       " 'to',\n",
       " 'me',\n",
       " 'what',\n",
       " 'would',\n",
       " 'you',\n",
       " 'say',\n",
       " 'your',\n",
       " 'greatest',\n",
       " 'weakness',\n",
       " 'is',\n",
       " 'i',\n",
       " 'said',\n",
       " 'i',\n",
       " 'think',\n",
       " 'id',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'my',\n",
       " 'listening',\n",
       " 'skills',\n",
       " 'are',\n",
       " 'my',\n",
       " 'greatest',\n",
       " 'strength',\n",
       " 'who',\n",
       " 'was',\n",
       " 'the',\n",
       " 'knight',\n",
       " 'that',\n",
       " 'invented',\n",
       " 'the',\n",
       " 'round',\n",
       " 'table',\n",
       " 'sir',\n",
       " 'cumference',\n",
       " 'via',\n",
       " 'friend',\n",
       " 'who',\n",
       " 'got',\n",
       " 'this',\n",
       " 'from',\n",
       " 'a',\n",
       " 'street',\n",
       " 'performance',\n",
       " 'group',\n",
       " 'in',\n",
       " 'the',\n",
       " 'england',\n",
       " 'area',\n",
       " 'of',\n",
       " 'epcot',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'german',\n",
       " 'air',\n",
       " 'force',\n",
       " 'eat',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " 'during',\n",
       " 'ww2',\n",
       " 'luftwaffles',\n",
       " 'i',\n",
       " 'the',\n",
       " 'shell',\n",
       " 'off',\n",
       " 'a',\n",
       " 'snail',\n",
       " 'yesterday',\n",
       " 'youd',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'move',\n",
       " 'faster',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'really',\n",
       " 'kinda',\n",
       " 'sluggish',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'number',\n",
       " 'zero',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'number',\n",
       " 'eight',\n",
       " 'nice',\n",
       " 'belt',\n",
       " 'whats',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'a',\n",
       " 'centipede',\n",
       " 'with',\n",
       " 'sore',\n",
       " 'feet',\n",
       " 'a',\n",
       " 'giraffe',\n",
       " 'with',\n",
       " 'a',\n",
       " 'sore',\n",
       " 'throat',\n",
       " 'whats',\n",
       " 'red',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'for',\n",
       " 'your',\n",
       " 'teeth',\n",
       " 'a',\n",
       " 'brick',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'chicken',\n",
       " 'cross',\n",
       " 'the',\n",
       " 'playground',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'the',\n",
       " 'other',\n",
       " 'slide',\n",
       " 'did',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'about',\n",
       " 'the',\n",
       " 'french',\n",
       " 'chef',\n",
       " 'who',\n",
       " 'committed',\n",
       " 'suicide',\n",
       " 'he',\n",
       " 'lost',\n",
       " 'the',\n",
       " 'huile',\n",
       " 'dolive',\n",
       " 'wanna',\n",
       " 'hear',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'about',\n",
       " 'unemployed',\n",
       " 'people',\n",
       " 'nevermind',\n",
       " 'they',\n",
       " 'dont',\n",
       " 'work',\n",
       " 'knock',\n",
       " 'knock',\n",
       " 'whos',\n",
       " 'there',\n",
       " 'boo',\n",
       " 'boo',\n",
       " 'who',\n",
       " 'dont',\n",
       " 'cry',\n",
       " 'its',\n",
       " 'only',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'how',\n",
       " 'much',\n",
       " 'did',\n",
       " 'the',\n",
       " 'skeleton',\n",
       " 'charge',\n",
       " 'for',\n",
       " 'his',\n",
       " 'excellent',\n",
       " 'legal',\n",
       " 'services',\n",
       " 'an',\n",
       " 'arm',\n",
       " 'and',\n",
       " 'a',\n",
       " 'leg',\n",
       " 'why',\n",
       " 'do',\n",
       " 'gorillas',\n",
       " 'have',\n",
       " 'such',\n",
       " 'big',\n",
       " 'nostrils',\n",
       " 'cos',\n",
       " 'they',\n",
       " 'got',\n",
       " 'big',\n",
       " 'fingers',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'a',\n",
       " 'siberian',\n",
       " 'husky',\n",
       " 'and',\n",
       " 'an',\n",
       " 'alaskan',\n",
       " 'husky',\n",
       " 'about',\n",
       " '1500',\n",
       " 'miles',\n",
       " 'what',\n",
       " 'do',\n",
       " 'vegan',\n",
       " 'zombies',\n",
       " 'eat',\n",
       " 'graaaiiinsss',\n",
       " 'whats',\n",
       " 'the',\n",
       " 'difference',\n",
       " 'between',\n",
       " 'a',\n",
       " 'thai',\n",
       " 'man',\n",
       " 'and',\n",
       " 'a',\n",
       " 'thai',\n",
       " 'woman',\n",
       " 'pls',\n",
       " 'help',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'call',\n",
       " 'a',\n",
       " 'car',\n",
       " 'that',\n",
       " 'eats',\n",
       " 'other',\n",
       " 'cars',\n",
       " 'a',\n",
       " 'carnivore',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'golfer',\n",
       " 'wear',\n",
       " 'two',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'pants',\n",
       " 'in',\n",
       " 'case',\n",
       " 'he',\n",
       " 'gets',\n",
       " 'a',\n",
       " 'hole',\n",
       " 'in',\n",
       " 'one',\n",
       " 'an',\n",
       " 'olympic',\n",
       " 'gymnast',\n",
       " 'walked',\n",
       " 'into',\n",
       " 'a',\n",
       " 'bar',\n",
       " 'she',\n",
       " 'didnt',\n",
       " 'get',\n",
       " 'a',\n",
       " 'medal',\n",
       " 'what',\n",
       " 'does',\n",
       " 'a',\n",
       " 'mexican',\n",
       " 'magician',\n",
       " 'make',\n",
       " 'for',\n",
       " 'breakfast',\n",
       " 'toasttahdahs',\n",
       " 'why',\n",
       " 'dont',\n",
       " 'bond',\n",
       " 'villains',\n",
       " 'feel',\n",
       " 'cold',\n",
       " 'in',\n",
       " 'the',\n",
       " 'winter',\n",
       " 'because',\n",
       " 'they',\n",
       " 'dress',\n",
       " 'in',\n",
       " 'lairs',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'figurine',\n",
       " 'say',\n",
       " 'when',\n",
       " 'the',\n",
       " 'boot',\n",
       " 'flew',\n",
       " 'past',\n",
       " 'her',\n",
       " 'protective',\n",
       " 'dome',\n",
       " 'that',\n",
       " 'was',\n",
       " 'a',\n",
       " 'cloche',\n",
       " 'call',\n",
       " 'what',\n",
       " 'was',\n",
       " 'carl',\n",
       " 'sagans',\n",
       " 'favorite',\n",
       " 'drink',\n",
       " 'cosmos',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'medical',\n",
       " 'term',\n",
       " 'for',\n",
       " 'owning',\n",
       " 'too',\n",
       " 'many',\n",
       " 'dogs',\n",
       " 'a',\n",
       " 'roverdosehttpiimgurcombtyf5ysjpg',\n",
       " 'knock',\n",
       " 'knock',\n",
       " 'whos',\n",
       " 'there',\n",
       " 'i',\n",
       " 'did',\n",
       " 'up',\n",
       " 'i',\n",
       " 'did',\n",
       " 'upwho',\n",
       " 'i',\n",
       " 'like',\n",
       " 'my',\n",
       " 'jokes',\n",
       " 'they',\n",
       " 'way',\n",
       " 'i',\n",
       " 'like',\n",
       " 'my',\n",
       " 'robots',\n",
       " 'killer',\n",
       " 'what',\n",
       " 'type',\n",
       " 'of',\n",
       " 'school',\n",
       " 'did',\n",
       " 'sherlock',\n",
       " 'holmes',\n",
       " 'go',\n",
       " 'to',\n",
       " 'elementary',\n",
       " 'my',\n",
       " 'friend',\n",
       " 'told',\n",
       " 'an',\n",
       " 'out',\n",
       " 'of',\n",
       " 'place',\n",
       " 'joke',\n",
       " 'about',\n",
       " 'police',\n",
       " 'searches',\n",
       " 'but',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'it',\n",
       " 'was',\n",
       " 'warranted',\n",
       " 'the',\n",
       " 'dalai',\n",
       " 'lama',\n",
       " 'walks',\n",
       " 'into',\n",
       " 'a',\n",
       " 'pizza',\n",
       " 'store',\n",
       " 'and',\n",
       " 'says',\n",
       " 'can',\n",
       " 'you',\n",
       " 'make',\n",
       " 'me',\n",
       " 'one',\n",
       " 'with',\n",
       " 'everything',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'vampire',\n",
       " 'use',\n",
       " 'mouthwash',\n",
       " 'because',\n",
       " 'he',\n",
       " 'had',\n",
       " 'bat',\n",
       " 'breath',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'corn',\n",
       " 'say',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'complemented',\n",
       " 'aww',\n",
       " 'shucks',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'green',\n",
       " 'grape',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'purple',\n",
       " 'grape',\n",
       " 'breathe',\n",
       " 'stupid',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'fall',\n",
       " 'break',\n",
       " 'off',\n",
       " 'from',\n",
       " 'all',\n",
       " 'the',\n",
       " 'other',\n",
       " 'seasons',\n",
       " 'because',\n",
       " 'it',\n",
       " 'wanted',\n",
       " 'autumnomy',\n",
       " 'if',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'fire',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'is',\n",
       " 'a',\n",
       " 'taylor',\n",
       " 'swift',\n",
       " 'fan',\n",
       " 'ill',\n",
       " 'say',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'you',\n",
       " 'were',\n",
       " 'trouble',\n",
       " 'when',\n",
       " 'you',\n",
       " 'clocked',\n",
       " 'in',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'do',\n",
       " 'if',\n",
       " 'a',\n",
       " 'cow',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " 'youre',\n",
       " 'driving',\n",
       " 'on',\n",
       " 'steer',\n",
       " 'clear',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'call',\n",
       " 'a',\n",
       " 'blind',\n",
       " 'legless',\n",
       " 'buck',\n",
       " 'no',\n",
       " 'eyedeer',\n",
       " 'edit',\n",
       " 'i',\n",
       " 'totally',\n",
       " 'messed',\n",
       " 'this',\n",
       " 'joke',\n",
       " 'up',\n",
       " 'please',\n",
       " 'give',\n",
       " 'me',\n",
       " 'another',\n",
       " 'chance',\n",
       " 'with',\n",
       " 'another',\n",
       " 'joke',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'get',\n",
       " 'for',\n",
       " 'the',\n",
       " 'women',\n",
       " 'who',\n",
       " 'has',\n",
       " 'everything',\n",
       " 'a',\n",
       " 'divorce',\n",
       " 'then',\n",
       " 'shell',\n",
       " 'only',\n",
       " 'have',\n",
       " 'half',\n",
       " 'of',\n",
       " 'everything',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'depressed',\n",
       " 'sausage',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'his',\n",
       " 'life',\n",
       " 'was',\n",
       " 'the',\n",
       " 'wurst',\n",
       " 'whats',\n",
       " 'a',\n",
       " 'dogs',\n",
       " 'favorite',\n",
       " 'mode',\n",
       " 'of',\n",
       " 'transportation',\n",
       " 'a',\n",
       " 'waggin',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'sand',\n",
       " 'dune',\n",
       " 'blush',\n",
       " 'because',\n",
       " 'the',\n",
       " 'sea',\n",
       " 'weed',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'the',\n",
       " 'tyrannical',\n",
       " 'peach',\n",
       " 'he',\n",
       " 'got',\n",
       " 'impeached',\n",
       " 'why',\n",
       " 'do',\n",
       " 'elephants',\n",
       " 'paint',\n",
       " 'their',\n",
       " 'toenails',\n",
       " 'red',\n",
       " 'so',\n",
       " 'they',\n",
       " 'can',\n",
       " 'hide',\n",
       " 'in',\n",
       " 'cherry',\n",
       " 'trees',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'an',\n",
       " 'elephant',\n",
       " 'in',\n",
       " 'a',\n",
       " 'cherry',\n",
       " 'tree',\n",
       " 'then',\n",
       " 'its',\n",
       " 'working',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'mexican',\n",
       " 'firecheif',\n",
       " 'name',\n",
       " 'his',\n",
       " 'kids',\n",
       " 'hose',\n",
       " 'a',\n",
       " 'and',\n",
       " 'hose',\n",
       " 'b',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " 'german',\n",
       " 'physicist',\n",
       " 'use',\n",
       " 'to',\n",
       " 'drink',\n",
       " 'his',\n",
       " 'beer',\n",
       " 'ein',\n",
       " 'stein',\n",
       " 'from',\n",
       " 'big',\n",
       " 'nate',\n",
       " 'as',\n",
       " 'told',\n",
       " 'by',\n",
       " 'my',\n",
       " 'kid',\n",
       " 'what',\n",
       " 'did',\n",
       " 'earth',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'other',\n",
       " 'planets',\n",
       " 'you',\n",
       " 'guys',\n",
       " 'have',\n",
       " 'no',\n",
       " 'life',\n",
       " 'one',\n",
       " 'time',\n",
       " 'we',\n",
       " 'ran',\n",
       " 'out',\n",
       " 'of',\n",
       " 'soap',\n",
       " 'so',\n",
       " 'we',\n",
       " 'had',\n",
       " 'to',\n",
       " 'use',\n",
       " 'hand',\n",
       " 'sanitizer',\n",
       " 'wanna',\n",
       " 'hear',\n",
       " 'a',\n",
       " 'dirty',\n",
       " 'joke',\n",
       " 'two',\n",
       " 'white',\n",
       " 'stallions',\n",
       " 'fell',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mud',\n",
       " 'what',\n",
       " 'did',\n",
       " 'one',\n",
       " 'frog',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " 'other',\n",
       " 'frog',\n",
       " 'times',\n",
       " 'fun',\n",
       " 'when',\n",
       " 'youre',\n",
       " 'having',\n",
       " 'flies',\n",
       " 'why',\n",
       " 'did',\n",
       " 'the',\n",
       " 'boy',\n",
       " 'take',\n",
       " 'a',\n",
       " 'pencil',\n",
       " 'and',\n",
       " 'paper',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'he',\n",
       " 'was',\n",
       " 'told',\n",
       " 'to',\n",
       " 'draw',\n",
       " 'the',\n",
       " 'curtains',\n",
       " 'before',\n",
       " 'going',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'clean',\n",
       " 'joke',\n",
       " 'about',\n",
       " 'sorority',\n",
       " 'girls',\n",
       " 'why',\n",
       " 'do',\n",
       " 'sorority',\n",
       " 'girls',\n",
       " 'only',\n",
       " 'travel',\n",
       " 'in',\n",
       " 'odd',\n",
       " 'numbered',\n",
       " 'groups',\n",
       " 'because',\n",
       " 'they',\n",
       " 'cant',\n",
       " 'even',\n",
       " 'what',\n",
       " 'did',\n",
       " 'the',\n",
       " '8',\n",
       " 'say',\n",
       " 'to',\n",
       " 'the',\n",
       " '0',\n",
       " 'hey',\n",
       " 'fatty',\n",
       " 'knock',\n",
       " 'knock',\n",
       " 'whos',\n",
       " 'there',\n",
       " 'sombrero',\n",
       " 'sombrero',\n",
       " 'who',\n",
       " 'sombrerover',\n",
       " 'the',\n",
       " 'rainbow',\n",
       " 'im',\n",
       " 'reading',\n",
       " 'a',\n",
       " 'book',\n",
       " 'about',\n",
       " 'antigravity',\n",
       " 'its',\n",
       " 'impossible',\n",
       " 'to',\n",
       " 'put',\n",
       " 'down',\n",
       " 'what',\n",
       " 'name',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset(4)\n",
    "dataset.load_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8815e-56b9-451f-b4ee-e1ec7273145c",
   "metadata": {
    "id": "37d8815e-56b9-451f-b4ee-e1ec7273145c"
   },
   "source": [
    "b) Now, complete the implementation of the RNN model.\n",
    "\n",
    "You'll need to use all the model components defined in `__init__`: [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), and the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) output layer. These are all subclasses of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "Torch Modules are objects that hold the layer's weights and biases (called parameters, accessed by `model.parameters()`) and keep track of a bunch of metadata, like what device the weights are on or what precision they're stored at. Each Module can have parts that are themselves Modules. The easiest way to combine Modules is with [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "\n",
    "Every Module must have a `forward` method. This defines what the Module does with its input and returns as output. You can access `forward` simply by calling the Module, for example `output = self.rnn(input)`. This is the preferred way to write it.\n",
    "\n",
    "Hint: unlike the one we saw in the tutorial, this RNN has multiple layers. Think about what this means for the shape of the hidden state. You might not want to use Sequential as RNN has multiple inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c0cf6-d662-4dcb-a8dc-298d2c92d347",
   "metadata": {
    "id": "4d6c0cf6-d662-4dcb-a8dc-298d2c92d347"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, n_vocab)\n",
    "\n",
    "    def init_state(self, sequence_length: int) -> Tensor:\n",
    "        \"\"\"Returns the initial state hidden state (all zeros), with the correct shape.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return torch.zeros(self.num_layers, sequence_length, self.hidden_size)\n",
    "\n",
    "    def forward(self, inputs: Tensor, prev_hidden_state: Tensor) -> (Tensor, Tensor):\n",
    "        \"\"\"Compute the logits and next_hidden_state.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(inputs)  # Convert inputs to embeddings\n",
    "        rnn_out, next_hidden_state = self.rnn(embedded, prev_hidden_state)  # Pass through RNN\n",
    "        logits = self.fc(rnn_out)  # Compute logits from RNN outputs\n",
    "        return logits, next_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b66e0-16db-494f-ae56-bb252f2ae218",
   "metadata": {
    "id": "0d1b66e0-16db-494f-ae56-bb252f2ae218"
   },
   "outputs": [],
   "source": [
    "model = RNNModel(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58e452-8530-40d4-8208-694525c7c161",
   "metadata": {
    "id": "8c58e452-8530-40d4-8208-694525c7c161"
   },
   "source": [
    "c) Write a function that counts the total number of parameters and total number of trainable parameters of a model.\n",
    "\n",
    "Refer to the [torch documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e22b4-570c-47d9-b2ab-74e73e337d44",
   "metadata": {
    "id": "263e22b4-570c-47d9-b2ab-74e73e337d44"
   },
   "outputs": [],
   "source": [
    "def count_params(model: nn.Module) -> (int, int):\n",
    "    # Count all parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    # Count trainable parameters\n",
    "    n_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return n_params, n_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b985f9-f444-4e5b-83e7-ab205838d669",
   "metadata": {
    "id": "79b985f9-f444-4e5b-83e7-ab205838d669"
   },
   "source": [
    "d) Complete the training loop and train the model for 10 epochs. Store the training loss in a list. You will probably want to have an inner loop that loops over batches.\n",
    "\n",
    "Hint: refer to the documentation of the `DataLoader`, `CrossEntropyLoss` and `Optimizer` classes. You might also need to use the `detach()` and `item()` methods to work with the loss tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529bd12-a84b-493a-819a-191beee539a4",
   "metadata": {
    "id": "5529bd12-a84b-493a-819a-191beee539a4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_rnn(dataset, model, sequence_length, batch_size, max_epochs) -> List[float]:\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_loss = 0.0  # To track the total loss for the epoch\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            # Initialize the hidden state for the batch\n",
    "            hidden_state = model.init_state(sequence_length)\n",
    "\n",
    "            # Reset the gradients from the previous step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            logits, _ = model(inputs, hidden_state)\n",
    "\n",
    "            # Reshape logits and targets for CrossEntropyLoss\n",
    "            logits = logits.view(-1, logits.size(-1))  # Shape: (batch_size * seq_len, vocab_size)\n",
    "            targets = targets.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss for this batch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Compute the average loss for the epoch\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        loss_list.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{max_epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784e538-36ae-4b06-b285-ee48459cb49c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6784e538-36ae-4b06-b285-ee48459cb49c",
    "outputId": "3885a840-850b-4848-dacd-bbbfcdf56930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 7.0807\n",
      "Epoch 2/10, Loss: 6.3801\n",
      "Epoch 3/10, Loss: 6.0680\n",
      "Epoch 4/10, Loss: 5.7919\n",
      "Epoch 5/10, Loss: 5.5440\n",
      "Epoch 6/10, Loss: 5.3075\n",
      "Epoch 7/10, Loss: 5.0839\n",
      "Epoch 8/10, Loss: 4.8692\n",
      "Epoch 9/10, Loss: 4.6686\n",
      "Epoch 10/10, Loss: 4.4749\n"
     ]
    }
   ],
   "source": [
    "train_loss_rnn = train_rnn(dataset, model, 4, 256, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f533ddb-6199-4ec4-9fe8-3433191d94a5",
   "metadata": {
    "id": "9f533ddb-6199-4ec4-9fe8-3433191d94a5"
   },
   "source": [
    "e) Complete the function to generate output from the model using 1. argmax (greedy) decoding 2. softmax decoding. Generate some sample outputs with each and discuss briefly.\n",
    "\n",
    "Hint: torch already has a builtin function for getting the softmax of a tensor, which you may use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c826b3-6073-4425-be68-bffc6d43c5e6",
   "metadata": {
    "id": "75c826b3-6073-4425-be68-bffc6d43c5e6"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from typing import List\n",
    "\n",
    "def predict_rnn_argmax(dataset: Dataset, model: nn.Module, text: str, next_words=100) -> List[str]:\n",
    "    model.eval()\n",
    "    words = text.split()  # Split the input text into words\n",
    "    hidden_state = model.init_state(1)  # Initialize the hidden state with batch size 1\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Convert words to indices\n",
    "        input_indices = torch.tensor([[dataset.word_to_index[word] for word in words[-model.num_layers:]]], dtype=torch.long)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits, hidden_state = model(input_indices, hidden_state)\n",
    "\n",
    "        # Take the last output and apply argmax to get the predicted word index\n",
    "        next_word_index = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "        # Convert the predicted index back to a word\n",
    "        next_word = dataset.index_to_word[next_word_index]\n",
    "\n",
    "        # Append the predicted word to the list of words\n",
    "        words.append(next_word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def predict_rnn_softmax(dataset: Dataset, model: nn.Module, text: str, next_words=100) -> List[str]:\n",
    "    model.eval()\n",
    "    words = text.split()  # Split the input text into words\n",
    "    hidden_state = model.init_state(1)  # Initialize the hidden state with batch size 1\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Convert words to indices\n",
    "        input_indices = torch.tensor([[dataset.word_to_index[word] for word in words[-model.num_layers:]]], dtype=torch.long)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        logits, hidden_state = model(input_indices, hidden_state)\n",
    "\n",
    "        # Take the last output and apply softmax to get probabilities\n",
    "        probabilities = softmax(logits[0, -1], dim=-1)\n",
    "\n",
    "        # Sample the next word index based on probabilities\n",
    "        next_word_index = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        # Convert the predicted index back to a word\n",
    "        next_word = dataset.index_to_word[next_word_index]\n",
    "\n",
    "        # Append the predicted word to the list of words\n",
    "        words.append(next_word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1ea38-6a51-428c-8c0c-02e12a616ce5",
   "metadata": {
    "id": "f9b1ea38-6a51-428c-8c0c-02e12a616ce5"
   },
   "source": [
    "f) Implement `LSTMModel`, `train_lstm`, `predict_lstm_argmax` and `predict_lstm_softmax`. Train the model using the same settings and plot both training loss curves together. Briefly discuss the differences in the model architectures and performance. Which model performs better and what are possible causes? What are the limitations of the model?\n",
    "\n",
    "Hint: use the `torch.nn.LSTM` class. You can do almost everything as with RNN, but take into account that an LSTM has **two** hidden states.\n",
    "\n",
    "Hint: You might not necessarily see that LSTM performs better even if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b3da15-96ea-4b58-af31-db8a65873aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 6.5807\n",
      "Epoch 2/10, Loss: 5.3203\n",
      "Epoch 3/10, Loss: 4.5033\n",
      "Epoch 4/10, Loss: 3.7784\n",
      "Epoch 5/10, Loss: 3.1868\n",
      "Epoch 6/10, Loss: 2.7267\n",
      "Epoch 7/10, Loss: 2.3709\n",
      "Epoch 8/10, Loss: 2.0883\n",
      "Epoch 9/10, Loss: 1.8607\n",
      "Epoch 10/10, Loss: 1.6774\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPTElEQVR4nO3deVhUZcMG8HsWGPZh3xQBEWVxQwG3XMp9K5dCTQu1UhO3yvbPMivNSl9Ly61CM3df13LfEzVQURFwQVRQQFSWYV9mzvcHOW8kKuLAmWHu33XNdTlnmbkHzLk75znnkQiCIICIiIhID0nFDkBERET0MCwqREREpLdYVIiIiEhvsagQERGR3mJRISIiIr3FokJERER6i0WFiIiI9BaLChEREektFhUiIiLSWywqRAZu9OjR8PLyqtG+M2fOhEQi0W0gIiIdYlEhqiUSiaRaj8OHD4sdVRSjR4+GlZWV2DGqbcuWLejbty8cHR1hamoKd3d3hIWF4eDBg2JHI6rXJJzrh6h2/Pbbb5We//rrr9i3bx9WrVpVaXnPnj3h4uJS4/cpKyuDRqOBQqF44n3Ly8tRXl4OMzOzGr9/TY0ePRqbNm1Cfn5+nb/3kxAEAWPHjsWKFSsQFBSEF198Ea6urkhPT8eWLVtw+vRpREVFoWPHjmJHJaqX5GIHIKqvRo0aVen5yZMnsW/fvgeW/1thYSEsLCyq/T4mJiY1ygcAcrkccjn/GXiUefPmYcWKFZg2bRrmz59f6VTZxx9/jFWrVunkZygIAoqLi2Fubv7Ur0VUn/DUD5GIunXrhubNm+P06dPo0qULLCws8NFHHwEAtm3bhv79+8Pd3R0KhQI+Pj74/PPPoVarK73Gv8eoXL9+HRKJBN9++y2WLVsGHx8fKBQKhISEICYmptK+VY1RkUgkmDRpErZu3YrmzZtDoVAgMDAQu3fvfiD/4cOHERwcDDMzM/j4+GDp0qU6H/eyceNGtG3bFubm5nB0dMSoUaNw69atSttkZGRgzJgxaNiwIRQKBdzc3PDCCy/g+vXr2m1OnTqF3r17w9HREebm5vD29sbYsWMf+d5FRUWYM2cO/Pz88O2331b5uV555RWEhoYCePiYnxUrVkAikVTK4+XlhQEDBmDPnj0IDg6Gubk5li5diubNm+PZZ5994DU0Gg0aNGiAF198sdKyBQsWIDAwEGZmZnBxccH48eORnZ39yM9FZEj4v1JEIrt37x769u2L4cOHY9SoUdrTQCtWrICVlRXefvttWFlZ4eDBg/jkk0+gUqnwzTffPPZ116xZg7y8PIwfPx4SiQRff/01hgwZguTk5McehTl27Bg2b96MiRMnwtraGt9//z2GDh2KlJQUODg4AABiY2PRp08fuLm54bPPPoNarcasWbPg5OT09D+Uv61YsQJjxoxBSEgI5syZg9u3b+O7775DVFQUYmNjYWtrCwAYOnQo4uPjMXnyZHh5eSEzMxP79u1DSkqK9nmvXr3g5OSEDz74ALa2trh+/To2b9782J9DVlYWpk2bBplMprPPdd+lS5cwYsQIjB8/Hm+88QaaNWuGYcOGYebMmcjIyICrq2ulLGlpaRg+fLh22fjx47U/oylTpuDatWtYtGgRYmNjERUV9VRH24j0hkBEdSIiIkL4939yXbt2FQAIS5YseWD7wsLCB5aNHz9esLCwEIqLi7XLwsPDBU9PT+3za9euCQAEBwcHISsrS7t827ZtAgBhx44d2mWffvrpA5kACKampkJSUpJ22blz5wQAwsKFC7XLBg4cKFhYWAi3bt3SLrty5Yogl8sfeM2qhIeHC5aWlg9dX1paKjg7OwvNmzcXioqKtMt///13AYDwySefCIIgCNnZ2QIA4Ztvvnnoa23ZskUAIMTExDw21z999913AgBhy5Yt1dq+qp+nIAhCZGSkAEC4du2adpmnp6cAQNi9e3elbS9duvTAz1oQBGHixImClZWV9u/Fn3/+KQAQVq9eXWm73bt3V7mcyFDx1A+RyBQKBcaMGfPA8n+OVcjLy8Pdu3fRuXNnFBYW4uLFi4993WHDhsHOzk77vHPnzgCA5OTkx+7bo0cP+Pj4aJ+3bNkSNjY22n3VajX279+PQYMGwd3dXbtdkyZN0Ldv38e+fnWcOnUKmZmZmDhxYqXBvv3794efnx/++OMPABU/J1NTUxw+fPihpzzuH3n5/fffUVZWVu0MKpUKAGBtbV3DT/Fo3t7e6N27d6VlTZs2RevWrbF+/XrtMrVajU2bNmHgwIHavxcbN26EUqlEz549cffuXe2jbdu2sLKywqFDh2olM1FdY1EhElmDBg1gamr6wPL4+HgMHjwYSqUSNjY2cHJy0g7Ezc3NfezrNmrUqNLz+6WlOuMX/r3v/f3v75uZmYmioiI0adLkge2qWlYTN27cAAA0a9bsgXV+fn7a9QqFAnPnzsWuXbvg4uKCLl264Ouvv0ZGRoZ2+65du2Lo0KH47LPP4OjoiBdeeAGRkZEoKSl5ZAYbGxsAFUWxNnh7e1e5fNiwYYiKitKOxTl8+DAyMzMxbNgw7TZXrlxBbm4unJ2d4eTkVOmRn5+PzMzMWslMVNdYVIhEVtVVHjk5OejatSvOnTuHWbNmYceOHdi3bx/mzp0LoGIQ5eM8bEyFUI07EjzNvmKYNm0aLl++jDlz5sDMzAwzZsyAv78/YmNjAVQMEN60aRNOnDiBSZMm4datWxg7dizatm37yMuj/fz8AABxcXHVyvGwQcT/HgB938Ou8Bk2bBgEQcDGjRsBABs2bIBSqUSfPn2022g0Gjg7O2Pfvn1VPmbNmlWtzET6jkWFSA8dPnwY9+7dw4oVKzB16lQMGDAAPXr0qHQqR0zOzs4wMzNDUlLSA+uqWlYTnp6eACoGnP7bpUuXtOvv8/HxwTvvvIO9e/fiwoULKC0txbx58ypt0759e3z55Zc4deoUVq9ejfj4eKxbt+6hGZ555hnY2dlh7dq1Dy0b/3T/95OTk1Np+f2jP9Xl7e2N0NBQrF+/HuXl5di8eTMGDRpU6V45Pj4+uHfvHjp16oQePXo88GjVqtUTvSeRvmJRIdJD949o/PMIRmlpKX788UexIlUik8nQo0cPbN26FWlpadrlSUlJ2LVrl07eIzg4GM7OzliyZEmlUzS7du1CYmIi+vfvD6DivjPFxcWV9vXx8YG1tbV2v+zs7AeOBrVu3RoAHnn6x8LCAu+//z4SExPx/vvvV3lE6bfffkN0dLT2fQHg6NGj2vUFBQVYuXJldT+21rBhw3Dy5En88ssvuHv3bqXTPgAQFhYGtVqNzz///IF9y8vLHyhLRIaKlycT6aGOHTvCzs4O4eHhmDJlCiQSCVatWqVXp15mzpyJvXv3olOnTnjzzTehVquxaNEiNG/eHGfPnq3Wa5SVleGLL754YLm9vT0mTpyIuXPnYsyYMejatStGjBihvTzZy8sLb731FgDg8uXL6N69O8LCwhAQEAC5XI4tW7bg9u3b2kt5V65ciR9//BGDBw+Gj48P8vLysHz5ctjY2KBfv36PzPjuu+8iPj4e8+bNw6FDh7R3ps3IyMDWrVsRHR2N48ePAwB69eqFRo0a4bXXXsO7774LmUyGX375BU5OTkhJSXmCn25FEZk+fTqmT58Oe3t79OjRo9L6rl27Yvz48ZgzZw7Onj2LXr16wcTEBFeuXMHGjRvx3XffVbrnCpHBEvGKIyKj8rDLkwMDA6vcPioqSmjfvr1gbm4uuLu7C++9956wZ88eAYBw6NAh7XYPuzy5qst1AQiffvqp9vnDLk+OiIh4YF9PT08hPDy80rIDBw4IQUFBgqmpqeDj4yP89NNPwjvvvCOYmZk95KfwP+Hh4QKAKh8+Pj7a7davXy8EBQUJCoVCsLe3F0aOHCncvHlTu/7u3btCRESE4OfnJ1haWgpKpVJo166dsGHDBu02Z86cEUaMGCE0atRIUCgUgrOzszBgwADh1KlTj81536ZNm4RevXoJ9vb2glwuF9zc3IRhw4YJhw8frrTd6dOnhXbt2gmmpqZCo0aNhPnz5z/08uT+/fs/8j07deokABBef/31h26zbNkyoW3btoK5ublgbW0ttGjRQnjvvfeEtLS0an82In3GuX6ISKcGDRqE+Ph4XLlyRewoRFQPcIwKEdVYUVFRpedXrlzBzp070a1bN3ECEVG9wyMqRFRjbm5uGD16NBo3bowbN25g8eLFKCkpQWxsLHx9fcWOR0T1AAfTElGN9enTB2vXrkVGRgYUCgU6dOiA2bNns6QQkc7wiAoRERHpLY5RISIiIr3FokJERER6y6DHqGg0GqSlpcHa2vqhc2wQERGRfhEEAXl5eXB3d4dU+uhjJgZdVNLS0uDh4SF2DCIiIqqB1NRUNGzY8JHbGHRRsba2BlDxQe9Px05ERET6TaVSwcPDQ/s9/igGXVTun+6xsbFhUSEiIjIw1Rm2wcG0REREpLdYVIiIiEhvsagQERGR3jLoMSpERCQOjUaD0tJSsWOQnjIxMYFMJtPJa7GoEBHREyktLcW1a9eg0WjEjkJ6zNbWFq6urk99nzMWFSIiqjZBEJCeng6ZTAYPD4/H3qyLjI8gCCgsLERmZiaAilnWnwaLChERVVt5eTkKCwvh7u4OCwsLseOQnjI3NwcAZGZmwtnZ+alOA7EKExFRtanVagCAqampyElI390vsmVlZU/1OiwqRET0xDi/Gj2Orv6OsKgQERGR3mJRISIiqgEvLy8sWLCg2tsfPnwYEokEOTk5tZapPmJRISKiek0ikTzyMXPmzBq9bkxMDMaNG1ft7Tt27Ij09HQolcoavV911bdCxKt+HiIhTQVbCxO425qLHYWIiJ5Cenq69s/r16/HJ598gkuXLmmXWVlZaf8sCALUajXk8sd/PTo5OT1RDlNTU7i6uj7RPsQjKlVaEXUNAxb+idk7E8WOQkRET8nV1VX7UCqVkEgk2ucXL16EtbU1du3ahbZt20KhUODYsWO4evUqXnjhBbi4uMDKygohISHYv39/pdf996kfiUSCn376CYMHD4aFhQV8fX2xfft27fp/H+lYsWIFbG1tsWfPHvj7+8PKygp9+vSpVKzKy8sxZcoU2NrawsHBAe+//z7Cw8MxaNCgGv88srOz8eqrr8LOzg4WFhbo27cvrly5ol1/48YNDBw4EHZ2drC0tERgYCB27typ3XfkyJFwcnKCubk5fH19ERkZWeMs1cGiUoUQb3sAwO/n0/FX8j2R0xAR6S9BEFBYWi7KQxAEnX2ODz74AF999RUSExPRsmVL5Ofno1+/fjhw4ABiY2PRp08fDBw4ECkpKY98nc8++wxhYWE4f/48+vXrh5EjRyIrK+uh2xcWFuLbb7/FqlWrcPToUaSkpGD69Ona9XPnzsXq1asRGRmJqKgoqFQqbN269ak+6+jRo3Hq1Cls374dJ06cgCAI6Nevn/Yy4oiICJSUlODo0aOIi4vD3LlztUedZsyYgYSEBOzatQuJiYlYvHgxHB0dnyrP4/DUTxUC3ZUYHtoIa/5KwWc7ErBj8jOQSXkpHhHRvxWVqRHwyR5R3jthVm9YmOrma2zWrFno2bOn9rm9vT1atWqlff75559jy5Yt2L59OyZNmvTQ1xk9ejRGjBgBAJg9eza+//57REdHo0+fPlVuX1ZWhiVLlsDHxwcAMGnSJMyaNUu7fuHChfjwww8xePBgAMCiRYu0Rzdq4sqVK9i+fTuioqLQsWNHAMDq1avh4eGBrVu34qWXXkJKSgqGDh2KFi1aAAAaN26s3T8lJQVBQUEIDg4GUHFUqbbxiMpDvNOzKazN5EhIV2F9TKrYcYiIqBbd/+K9Lz8/H9OnT4e/vz9sbW1hZWWFxMTExx5RadmypfbPlpaWsLGx0d5KvioWFhbakgJU3G7+/va5ubm4ffs2QkNDtetlMhnatm37RJ/tnxITEyGXy9GuXTvtMgcHBzRr1gyJiRXDHaZMmYIvvvgCnTp1wqefforz589rt33zzTexbt06tG7dGu+99x6OHz9e4yzVxSMqD+FgpcBbPZpi1u8J+HbvJfRv6QaluYnYsYiI9Iq5iQwJs3qL9t66YmlpWen59OnTsW/fPnz77bdo0qQJzM3N8eKLLz52xmgTk8rfExKJ5JGTN1a1vS5PadXE66+/jt69e+OPP/7A3r17MWfOHMybNw+TJ09G3759cePGDezcuRP79u1D9+7dERERgW+//bbW8vCIyiO80sETTZytkFVQiu/2X3n8DkRERkYikcDCVC7KozbvjhsVFYXRo0dj8ODBaNGiBVxdXXH9+vVae7+qKJVKuLi4ICYmRrtMrVbjzJkzNX5Nf39/lJeX46+//tIuu3fvHi5duoSAgADtMg8PD0yYMAGbN2/GO++8g+XLl2vXOTk5ITw8HL/99hsWLFiAZcuW1ThPdfCIyiOYyKSYMSAA4b9E49cT1/FyOw80cbYWOxYREdUyX19fbN68GQMHDoREIsGMGTMeeWSktkyePBlz5sxBkyZN4Ofnh4ULFyI7O7taJS0uLg7W1v/7zpJIJGjVqhVeeOEFvPHGG1i6dCmsra3xwQcfoEGDBnjhhRcAANOmTUPfvn3RtGlTZGdn49ChQ/D39wcAfPLJJ2jbti0CAwNRUlKC33//XbuutrCoPEbXpk7o4e+M/YmZmPV7IlaOCeEcF0RE9dz8+fMxduxYdOzYEY6Ojnj//fehUqnqPMf777+PjIwMvPrqq5DJZBg3bhx69+5drdmIu3TpUum5TCZDeXk5IiMjMXXqVAwYMAClpaXo0qULdu7cqT0NpVarERERgZs3b8LGxgZ9+vTBf/7zHwAV94L58MMPcf36dZibm6Nz585Yt26d7j/4P0gEsU+GPQWVSgWlUonc3FzY2NjU2vtcv1uAnv85gjK1gJ/Dg9Hd36XW3ouISJ8VFxfj2rVr8Pb2hpmZmdhxjI5Go4G/vz/CwsLw+eefix3nkR71d+VJvr85RqUavBwtMfYZbwDA578noKRcLXIiIiIyBjdu3MDy5ctx+fJlxMXF4c0338S1a9fw8ssvix2tzrCoVNPk53zhZK3A9XuFWBF1Xew4RERkBKRSKVasWIGQkBB06tQJcXFx2L9/f62PC9EnHKNSTVYKOd7r3QzvbjqPhQeTMLhNAzhb87AnERHVHg8PD0RFRYkdQ1Q8ovIEhrZpiFYNlcgvKcc3uy89fgciIiJ6KiwqT0AqleDT5wMBABtP38S51BxxAxERicSAr8OgOqKrvyMsKk+oTSM7DAlqAACYuSOe/7ESkVG5f1ns4+7QSlRYWAjgwbvvPimOUamB9/v6YXd8BmJTcrD17C0MDmoodiQiojohl8thYWGBO3fuwMTEBFIp/3+XKhMEAYWFhcjMzIStrW217vnyKCwqNeBiY4aIZ5vgmz2X8NWui+gV4ApLBX+URFT/SSQSuLm54dq1a7hx44bYcUiP2drawtXV9alfh9+uNfTaM95YH5OKlKxC/Hg4Ce/29hM7EhFRnTA1NYWvry9P/9BDmZiYPPWRlPtYVGrIzESGj/v7Y/yq01j+5zUMC26ERg4WYsciIqoTUqmUd6alOsGTi0+hV4ALOjVxQGm5Bl/uTBA7DhERUb3DovIUJBIJPhkQCJlUgj3xtxGVdFfsSERERPUKi8pTauZqjVHtGgEAZu1IQLm67qcBJyIiqq9YVHTgrZ5NYWthgku387D6rxSx4xAREdUbLCo6YGthind6NgUAzN93GdkFHAlPRESkCywqOjIitBH8XK2RW1SG+fsuix2HiIioXmBR0RG5TIpPBgYAAFb/dQMXM1QiJyIiIjJ8LCo61NHHEX2bu0IjAJ9tT+A8QERERE+JRUXHPurnD1O5FCeS72FPfIbYcYiIiAwai4qOedhbYHyXxgCAL/5IRHGZWuREREREhotFpRa82c0HrjZmuJldhJ/+TBY7DhERkcFiUakFFqZyfNivYpLCHw5dRXpukciJiIiIDBOLSi15vpU7gj3tUFSmxtxdF8WOQ0REZJBYVGqJRCLBpwMDIZEAW8+m4fSNLLEjERERGRwWlVrUoqESYW09AAAztydAo+HlykRERE+CRaWWTe/dDNYKOeJu5WLT6ZtixyEiIjIoLCq1zMlagSndfQEAX++5iLziMpETERERGQ7Ri8qtW7cwatQoODg4wNzcHC1atMCpU6fEjqVT4R290NjREnfzS7HwYJLYcYiIiAyGqEUlOzsbnTp1gomJCXbt2oWEhATMmzcPdnZ2YsbSOVO5FDMGVMwDFBl1Dcl38kVOREREZBjkYr753Llz4eHhgcjISO0yb29vERPVnmf9nNGtmRMOX7qDL/5IxC+jQ8SOREREpPdEPaKyfft2BAcH46WXXoKzszOCgoKwfPlyMSPVqhkDAiCXSnDwYiYOXcoUOw4REZHeE7WoJCcnY/HixfD19cWePXvw5ptvYsqUKVi5cmWV25eUlEClUlV6GBIfJyuM7ugFAPj89wSUlmvEDURERKTnRC0qGo0Gbdq0wezZsxEUFIRx48bhjTfewJIlS6rcfs6cOVAqldqHh4dHHSd+elN6+MLB0hTJdwrw64nrYschIiLSa6IWFTc3NwQEBFRa5u/vj5SUlCq3//DDD5Gbm6t9pKam1kVMnbIxM8G7vZsBAL7bfwV380tETkRERKS/RC0qnTp1wqVLlyotu3z5Mjw9PavcXqFQwMbGptLDEL0U7IHmDWyQV1KOeXsvPX4HIiIiIyVqUXnrrbdw8uRJzJ49G0lJSVizZg2WLVuGiIgIMWPVOpm0Yh4gAFgXk4oLt3JFTkRERKSfRC0qISEh2LJlC9auXYvmzZvj888/x4IFCzBy5EgxY9WJEC97PN/KHYIAfLYjHoLAeYCIiIj+TSIY8DekSqWCUqlEbm6uQZ4GSsspwnPzDqO4TIPvRwTh+VbuYkciIiKqdU/y/S36LfSNmbutOSZ2awIAmLMzEUWlapETERER6RcWFZGN69IYDWzNkZ5bjMVHroodh4iISK+wqIjMzESGj/v7AwCWHrmKm9mFIiciIiLSHywqeqBvc1e087ZHSbkGc3ZeFDsOERGR3mBR0QMSScXlylIJ8EdcOk4m3xM7EhERkV5gUdETAe42GBHaCADw2Y4EqDUGezEWERGRzrCo6JF3ejWDjZkciekqrIupehoBIiIiY8KiokfsLU3xVs+mAIBv91xCbmGZyImIiIjExaKiZ0a194SvsxWyC8uw4MBlseMQERGJikVFz5jIpPhkYMWM0r+euIErt/NETkRERCQeFhU91NnXCT0DXKDWCJj1ewLnASIiIqPFoqKn/q+/P0xlUvx55S72J2aKHYeIiEgULCp6ytPBEq919gYAfPFHAkrKOQ8QEREZHxYVPRbxbBM4Wytw414hfjl2Xew4REREdY5FRY9ZKeR4v48fAGDRwSvIVBWLnIiIiKhusajoucFBDdDKwxYFpWrM3X1J7DhERER1ikVFz0mlEsz8+3Ll/565ibOpOeIGIiIiqkMsKgYgqJEdhrRpAACYuT0eGs4DRERERoJFxUB80McPlqYynE3Nwdazt8SOQ0REVCdYVAyEs40ZIp5rAgD4atdF5JeUi5yIiIio9rGoGJDXnvGGp4MFMvNK8MOhJLHjEBER1ToWFQOikMvwcT9/AMDPf17DjXsFIiciIiKqXSwqBqZngAs6+zqiVK3BF38kih2HiIioVrGoGBiJRIJPBgRAJpVgX8Jt/HnljtiRiIiIag2LigHydbHGK+09AQCzdiSgTK0ROREREVHtYFExUG/1aAo7CxNcyczH6pM3xI5DRERUK1hUDJTSwgTv9GoGAJi/7zKyCkpFTkRERKR7LCoGbERoI/i5WkNVXI75+zgPEBER1T8sKgZMJpXg04GBAIA1f6UgIU0lciIiIiLdYlExcB18HNCvhSs0AjDr93gIAucBIiKi+oNFpR74sK8/FHIpTiZnYdeFDLHjEBER6QyLSj3gYW+B8V0aAwC+/CMRxWVqkRMRERHpBotKPTGhmw/clGa4lVOEZUeTxY5DRESkEywq9YSFqRwf9PUDAPx4OAlpOUUiJyIiInp6LCr1yPOt3BHsaYfiMg2+2nVR7DhERERPjUWlHpFIJJj5fCAkEmD7uTTEXM8SOxIREdFTYVGpZ5o3UGJYsAcA4LMd8VBreLkyEREZLhaVemh672awVshx4ZYKm06nih2HiIioxlhU6iFHKwWm9vAFAHyz5xJUxWUiJyIiIqoZFpV66tUOXmjsZIm7+aWYv/ey2HGIiIhqhEWlnjKVSzFjQAAAYMXx61gbnSJyIiIioifHolKPPdvMGRHP+gAAPt4Sh/0Jt0VORERE9GRYVOq56b2a4cW2DaERgElrz+BMSrbYkYiIiKqNRaWek0gkmDOkBbo1c0JxmQavrYjB1Tv5YsciIiKqFhYVI2Aik+KHl9ugZUMlsgvL8OrP0chUFYsdi4iI6LFYVIyEpUKOX0aHwMvBArdyihAeGYM8XrZMRER6jkXFiDhaKfDr2HZwtDJFYroKE347jdJyjdixiIiIHopFxcg0crBA5OhQWJrKEJV0D9M3noOGt9knIiI9xaJihFo0VGLxqLaQSyXYfi4Nc3Ylih2JiIioSiwqRqpLUyd8/WJLAMDyP6/hpz+TRU5ERET0IBYVIzakTUO838cPAPDFH4nYfi5N5ERERESVsagYuQldG2N0Ry8AwDsbzuJ40l1xAxEREf0Di4qRk0gkmDEgAP1auKJMLWDcqtOIT8sVOxYREREAFhUCIJNKMD+sNdp52yO/pByjI2OQmlUodiwiIiIWFapgZiLDsleD0czFGnfyShAeGY3sglKxYxERkZFjUSEtpbkJVowNgZvSDMl3CjB2ZQyKStVixyIiIiPGokKVuCnN8evYUCjNTRCbkoPJa8+gXM271xIRkThYVOgBvi7W+Ck8GAq5FPsTMzFj2wUIAu9eS0REdU/UojJz5kxIJJJKDz8/PzEj0d9CvOzx3fAgSCXA2uhUfHfgitiRiIjICIl+RCUwMBDp6enax7Fjx8SORH/r09wVn73QHACwYP8VrI1OETkREREZG7noAeRyuLq6ih2DHuKV9p64nVuMRYeS8PGWODhZKdAjwEXsWEREZCREP6Jy5coVuLu7o3Hjxhg5ciRSUh7+f+0lJSVQqVSVHlT73unVFC+1bQiNAExaewZnUrLFjkREREZC1KLSrl07rFixArt378bixYtx7do1dO7cGXl5eVVuP2fOHCiVSu3Dw8OjjhMbJ4lEgtlDWqBbMycUl2nw2ooYXL2TL3YsIiIyAhJBjy7nyMnJgaenJ+bPn4/XXnvtgfUlJSUoKSnRPlepVPDw8EBubi5sbGzqMqpRKiwtx4hlJ3HuZi4a2Jpjy8SOcLYxEzsWEREZGJVKBaVSWa3vb9FP/fyTra0tmjZtiqSkpCrXKxQK2NjYVHpQ3bEwleOX0SHwcrDArZwihEfGIK+4TOxYRERUj+lVUcnPz8fVq1fh5uYmdhR6CAcrBX4d2w6OVqZITFdhwm+nUVrOG8IREVHtELWoTJ8+HUeOHMH169dx/PhxDB48GDKZDCNGjBAzFj1GIwcLRI4OhaWpDFFJ9zB94zloNHpzBpGIiOoRUYvKzZs3MWLECDRr1gxhYWFwcHDAyZMn4eTkJGYsqoYWDZVYPKot5FIJtp9Lw+ydiWJHIiKiekivBtM+qScZjEO1Y0vsTby1/hwA4P/6++P1zo1FTkRERPrOYAfTkuEZHNQQH/StmPbgiz8Sse3sLZETERFRfcKiQk9tfJfGGN3RCwAwfeM5RCXdFTcQERHVGywq9NQkEgk+GRCA/i3cUKYWMH7VacSn5Yodi4iI6gEWFdIJqVSCeWGt0M7bHvkl5RgdGYPUrEKxYxERkYFjUSGdMTORYdmrwfBztcadvBKER0Yju6BU7FhERGTAWFRIp5TmJlgxJhTuSjMk3ynA2JUxKCpVix2LiIgMFIsK6Zyr0gwrx4ZCaW6C2JQcTF57BuVq3r2WiIieHIsK1QpfF2v8FB4MhVyK/YmZmLHtAgz4lj1ERCQSFhWqNSFe9vh+RBCkEmBtdCq+O3BF7EhERGRgWFSoVvUOdMWsF5oDABbsv4K10SkiJyIiIkPCokK1blR7T0x+rgkA4OMtcdifcFvkREREZChYVKhOvN2zKcKCG0IjAJPWnsHpG9liRyIiIgPAokJ1QiKR4MvBLfBsMycUl2nw2soYJGXmix2LiIj0HIsK1RkTmRQ/jGyDVh62yCksQ/gv0bitKhY7FhER6TEWFapTFqZy/BIeDG9HS9zKKcLoyBioisvEjkVERHqKRYXqnIOVAivHhMLRSoHEdBUmrDqNknLevZaIiB7EokKiaORggRVjQmBpKsPxq/cwfeN5aDS8IRwREVXGokKiad5AiSWvtIVcKsGOc2mYvTNR7EhERKRnWFRIVJ19nfDNSy0BAD8du4af/kwWOREREekTFhUS3eCghviwrx8A4Is/ErHt7C2RExERkb5gUSG9MK5LY4zp5AUAmL7xHKKS7oobiIiI9AKLCukFiUSCGf0D0L+lG8rUAsavOo34tFyxYxERkchYVEhvSKUSzA9rhfaN7ZFfUo7RkTFIzSoUOxYREYmIRYX0ikIuw7JXg+Hnao07eSUIj4xGdkGp2LGIiEgkLCqkd2zMTLBiTCga2Joj+U4Bxq6MQVEpbwhHRGSMWFRIL7kqzbBybAiU5iaITcnB5LVnUK7WiB2LiIjqGIsK6a0mztb4OTwYCrkU+xMz8fGWC7x7LRGRkWFRIb0W7GWP70cEQSoB1p9Kxcdb41hWiIiMCIsK6b3ega6YF9YKUgmwNjoVH25mWSEiMhYsKmQQBgc1xH+GtdYeWXn/v5zEkIjIGLCokMF4oXUDLBhecRpo4+mbeO+/56FmWSEiqtfkYgcgehLPt3KHBMC09Wex6fRNaAQB37zYCjKpROxoRERUC3hEhQzOwFbu+H54EGRSCTafuYXpG8/xyAoRUT3FokIGqX9LNywaEQS5VIItsbfw9oazvM8KEVE9xKJCBqtvCzcserkN5FIJtp1Nw1sbzrGsEBHVMywqZND6NHfFjyPbwEQmwY5zaZi6nkdWiIjqExYVMni9Al3x48i2MJFJ8Mf5dExddxZlLCtERPUCiwrVCz0DXLBkVFuYyqT4Iy4dU9bGsqwQEdUDLCpUb3T3d8HSVyrKyq4LGZi05gxKy1lWiIgMGYsK1SvP+jlj2attYSqXYk/8bUSwrBARGTQWFap3ujVzxvJXg2Eql2Jfwm1MXH0aJeVqsWMREVEN1KiopKam4ubNm9rn0dHRmDZtGpYtW6azYERPo2tTJ/z0ajAUcin2J2Zi4m9nWFaIiAxQjYrKyy+/jEOHDgEAMjIy0LNnT0RHR+Pjjz/GrFmzdBqQqKa6NHXCz+EhUMilOHAxExNWnUZxGcsKEZEhqVFRuXDhAkJDQwEAGzZsQPPmzXH8+HGsXr0aK1as0GU+oqfyjK8jIkeHwMxEikOX7mA8ywoRkUGpUVEpKyuDQqEAAOzfvx/PP/88AMDPzw/p6em6S0ekAx2bOCJydCjMTWQ4cvkO3vj1FMsKEZGBqFFRCQwMxJIlS/Dnn39i37596NOnDwAgLS0NDg4OOg1IpAsdfBwQOSYE5iYy/HnlLssKEZGBqFFRmTt3LpYuXYpu3bphxIgRaNWqFQBg+/bt2lNCRPqmfWMHrBgTAgvTirLy+spTKCplWSEi0mcSQRCEmuyoVquhUqlgZ2enXXb9+nVYWFjA2dlZZwEfRaVSQalUIjc3FzY2NnXynmT4Yq5nYfQv0SgoVaOjjwN+Cg+Ghalc7FhEREbjSb6/a3REpaioCCUlJdqScuPGDSxYsACXLl2qs5JCVFMhXvZYOTYUVgo5jl+9h7ErYlBYWi52LCIiqkKNisoLL7yAX3/9FQCQk5ODdu3aYd68eRg0aBAWL16s04BEtSH4H2XlZHIWRkfGoKCEZYWISN/UqKicOXMGnTt3BgBs2rQJLi4uuHHjBn799Vd8//33Og1IVFvaetrh19dCYa2QI/paFsawrBAR6Z0aFZXCwkJYW1sDAPbu3YshQ4ZAKpWiffv2uHHjhk4DEtWmNo3ssOr1drA2kyP6ehZGR0Yjn2WFiEhv1KioNGnSBFu3bkVqair27NmDXr16AQAyMzM5qJUMTmsPW/z2WkVZibmejfBfopFXXCZ2LCIiQg2LyieffILp06fDy8sLoaGh6NChA4CKoytBQUE6DUhUF1p52GL16+1gYybH6RvZePWXaKhYVoiIRFfjy5MzMjKQnp6OVq1aQSqt6DvR0dGwsbGBn5+fTkM+DC9PJl27cCsXI3/6C7lFZWjtYYtfXwuFjZmJ2LGIiOqVJ/n+rnFRue/+LMoNGzZ8mpepERYVqg0XbuVi1M9/IaewDK08bPHr2FAozVlWiIh0pdbvo6LRaDBr1iwolUp4enrC09MTtra2+Pzzz6HRaGoUmkhfNG+gxJrX28POwgTnUnPwys9/IbeQp4GIiMRQo6Ly8ccfY9GiRfjqq68QGxuL2NhYzJ49GwsXLsSMGTNqFOSrr76CRCLBtGnTarQ/kS4FuNtgzRvtYW9pivM37x9hKRU7FhGR0anRqR93d3csWbJEO2vyfdu2bcPEiRNx69atJ3q9mJgYhIWFwcbGBs8++ywWLFhQrf146odq28UMFUYu/wv3CkoR6G6D1a+3g62FqdixiIgMWq2f+snKyqpywKyfnx+ysrKe6LXy8/MxcuRILF++vNK8QUT6wM/VBmvHtYejlSni01R4eflfyC7gkRUiorpSo6LSqlUrLFq06IHlixYtQsuWLZ/otSIiItC/f3/06NGjJlGIal1TF2usfaM9HK0USEhX4eWf/kIWywoRUZ2o0ZSxX3/9Nfr374/9+/dr76Fy4sQJpKamYufOndV+nXXr1uHMmTOIiYmp1vYlJSUoKSnRPlepVE8WnKiGfF2ssW5cOwxf9hcS01V4eflJrH69HRysFGJHIyKq12p0RKVr1664fPkyBg8ejJycHOTk5GDIkCGIj4/HqlWrqvUaqampmDp1KlavXg0zM7Nq7TNnzhwolUrtw8PDoybxiWqkibM11o1rDydrBS5m5OHl5X/hbn7J43ckIqIae+r7qPzTuXPn0KZNG6jV6sduu3XrVgwePBgymUy7TK1WQyKRQCqVoqSkpNI6oOojKh4eHhxMS3Xq6p18jFh2Epl5JfB1tsKaNyrKCxERVU+tD6bVhe7duyMuLg5nz57VPoKDgzFy5EicPXv2gZICAAqFAjY2NpUeRHXNx8kK68a1h4uNAlcy8zFi+Ulk5hWLHYuIqF4SrahYW1ujefPmlR6WlpZwcHBA8+bNxYpFVC2NnaywflwHuCnNkJT59xEWFcsKEZGuiVZUiAydl6Ml1o1rD3elGa7eKcDw5SwrRES69kRjVIYMGfLI9Tk5OThy5Ei1xqjoAm/4Rvog5V4hRiw/iVs5RWjsaIk1b7SHq7J6A8SJiIxRrY1R+ecVN1U9PD098eqrrz5VeCJD08jBAuvGtUcDW3Mk3y3A8GUnkJ5bJHYsIqJ6QadX/dQ1HlEhfZKaVXFk5WZ2ETwdLLD2jfZwtzUXOxYRkd4xiKt+iOobD/uKIyse9ua4ca8Qw5dVnA4iIqKaY1Eh0qGGdhZYN64DGtlbICWrEMOXncDN7EKxYxERGSwWFSIda2BrjnXj2sPTwQKpWUUYvuwkUrNYVoiIaoJFhagWuP9dVrwcLHAzm2WFiKimWFSIaomb0hzrxnVAY0dL3MqpKCsp91hWiIieBIsKUS1yVZph7bj2aOx0v6ycwI17BWLHIiIyGCwqRLXMxcYM695oDx8nS6TlFuOlJSdwNjVH7FhERAaBRYWoDjjbVBxZaepihcy8EoQtPYFNp2+KHYuISO+xqBDVEWdrM/z3zY7o4e+C0nINpm88h1k7ElCu1ogdjYhIb7GoENUhazMTLHulLaZ09wUA/BJ1DeGR0cguKBU5GRGRfmJRIapjUqkEb/dsiiWj2sDCVIaopHt4/odjSExXiR2NiEjvsKgQiaRPczdsntgRjewrbgw35Mfj2BmXLnYsIiK9wqJCJCI/Vxtsn9QJzzRxRFGZGhNXn8G8vZeg0RjsXKFERDrFokIkMlsLU6wYE4LXn/EGACw8mIRxq04hr7hM5GREROJjUSHSA3KZFP83IADzw1rBVC7F/sRMDPohCsl38sWORkQkKhYVIj0ypE1DbJrQAW5KM1y9U4AXfojCoYuZYsciIhINiwqRnmnZ0BbbJnVCsKcd8orLMXZlDH48nARB4LgVIjI+LCpEesjZ2gxr3miPEaGNIAjA17svYfLaWBSVqsWORkRUp1hUiPSUqVyKOUNa4ItBzSGXSvD7+XQMXXwcqVmcgZmIjAeLCpGeG9XeE2veaA8HS1MkpKvwwg9ROHH1ntixiIjqBIsKkQEI9bbHjsnPoHkDG2QVlGLUz39h5fHrHLdCRPUeiwqRgXC3NcemCR0xqLU71BoBn26Px/v/PY+Sco5bIaL6i0WFyICYmcjwn2Gt8VE/P0glwIZTNzF82UlkqorFjkZEVCtYVIgMjEQiwbguPogcEwobMzliU3IwYOExxKZkix2NiEjnWFSIDFTXpk7YPukZ+DpbITOvBMOWnsSGU6lixyIi0ikWFSID5uVoiS0RndArwAWlag3e23QeM7fHo0ytETsaEZFOsKgQGTgrhRxLRrXFtB6+AIAVx6/j1Z+jkVVQKnIyIqKnx6JCVA9IpRJM69EUS19pC0tTGU4k38Pzi44hIU0ldjQioqfCokJUj/QOdMWWiE7wdLDAzewiDF18HH+cTxc7FhFRjbGoENUzTV2ssS2iEzr7OqKoTI2INWfwzZ6LUGt4czgiMjwsKkT1kK2FKSJHh2Bcl8YAgB8OXcUbv56CqrhM5GRERE+GRYWonpLLpPionz8WDGsNhVyKgxczMeiHKFy9ky92NCKiamNRIarnBgU1wKYJHeGmNEPynQIMWhSFgxdvix2LiKhaWFSIjECLhkpsn/QMQrzskFdSjtdWnsIPh5I4qSER6T0WFSIj4WStwOrX22Nku0YQBOCbPZcwaU0sCkvLxY5GRPRQLCpERsRULsWXg1tg9uAWMJFJ8EdcOob8eBypWYViRyMiqhKLCpERerldI6x5oz0crUxxMSMPzy86huNX74odi4joASwqREYqxMse2yc9gxYNlMguLMMrP0cjMuoax60QkV5hUSEyYu625tg4oQMGBzWAWiPgsx0JeHfTeRSXqcWORkQEgEWFyOiZmcgwP6wV/q+/P6QSYNPpmxi+7CRuq4rFjkZExKJCRIBEIsHrnRtj5dhQKM1NcDY1BwMXHsOZlGyxoxGRkWNRISKtzr5O2D6pE5q6WCEzrwTDl57EhphUsWMRkRFjUSGiSjwdLLF5Yif0DnRBqVqD9/57Hp9uu4AytUbsaERkhFhUiOgBVgo5Fo9si7d7NgUArDxxA6/8/Bfu5ZeInIyIjA2LChFVSSqVYEp3Xyx7pS0sTWU4mZyF5xdFIT4tV+xoRGREWFSI6JF6Bbpia0QneDlY4FZOEYYuPo4d59LEjkVERoJFhYgey9fFGtsinkGXpk4oLtNg8tpYzNmViJJy3m+FiGoXiwoRVYvSwgSRo0MwvktjAMDSI8no//0xRF/LEjkZEdVnLCpEVG0yqQQf9vPHjyPbwNHKFEmZ+QhbegIfbj6P3MIyseMRUT3EokJET6xfCzfsf7srhod4AADWRqei+/wj2H4ujXMFEZFOsagQUY3YWpjiq6EtsWF8B/g4WeJufgmmrI3F6MgYpGYVih2PiOoJFhUieiqh3vbYObUz3urRFKYyKY5cvoOe/zmCpUeuopw3iSOip8SiQkRPTSGXYWoPX+ya1hntvO1RXKbBnF0XMXBRFM6l5ogdj4gMGIsKEemMj5MV1o1rj69fbAmluQkS01UY9GMUZm6PR35JudjxiMgAsagQkU5JJBKEBXvgwDtdMai1OwQBWHH8OnrMO4I98RlixyMiA8OiQkS1wtFKgQXDg7DqtVA0srdAhqoY41edxvhVp5CRWyx2PCIyEKIWlcWLF6Nly5awsbGBjY0NOnTogF27dokZiYh0rLOvE/ZM64I3u/lALpVgT/xt9Jh/BCuPX4daw0uZiejRJIKINz3YsWMHZDIZfH19IQgCVq5ciW+++QaxsbEIDAx87P4qlQpKpRK5ubmwsbGpg8RE9DQuZqjw4eY4xKbkAABae9hizpAW8Hfjf79ExuRJvr9FLSpVsbe3xzfffIPXXnvtsduyqBAZHrVGwJq/buDr3ZeQV1IOmVSC1zt7Y1r3pjA3lYkdj4jqwJN8f+vNGBW1Wo1169ahoKAAHTp0qHKbkpISqFSqSg8iMiwyqQSvdPDC/ne6om9zV6g1ApYeSUavBUdw5PIdseMRkZ4RvajExcXBysoKCoUCEyZMwJYtWxAQEFDltnPmzIFSqdQ+PDw86jgtEemKi40ZFo9qi59eDYa70gypWUUI/yUaU9fF4m5+idjxiEhPiH7qp7S0FCkpKcjNzcWmTZvw008/4ciRI1WWlZKSEpSU/O8fMJVKBQ8PD576ITJw+SXlmLf3ElYevw6NACjNTfBRPz+EBXtAIpGIHY+IdMygx6j06NEDPj4+WLp06WO35RgVovrl/M0cfPDfOCSkV5zWDfW2x+zBLdDE2UrkZESkSwY5RuU+jUZT6agJERmPlg1tsX1SJ3zczx/mJjJEX8tCv+/+xH/2XUZJuVrseEQkAlGLyocffoijR4/i+vXriIuLw4cffojDhw9j5MiRYsYiIhHJZVK80aUx9r7VBc82c0KpWoPvDlxB3+/+xMnke2LHI6I6JmpRyczMxKuvvopmzZqhe/fuiImJwZ49e9CzZ08xYxGRHvCwt8Avo0Ow6OUgOFopkHynAMOXncR7m84hp7BU7HhEVEf0bozKk+AYFSLjkFtUhrm7L2LNXykAAEcrU8wYEIDnW7lzsC2RATLoMSpERP+mNDfB7MEtsHFCB/g6W+FufimmrjuLV3+JRsq9QrHjEVEtYlEhIoMR4mWPP6Z0xjs9m8JULsWfV+6i14IjWHz4KsrUGrHjEVEtYFEhIoNiKpdicndf7J7aGR0aO6C4TIO5uy9i4MJjiE3JFjseEekYiwoRGaTGTlZY80Y7fPtSK9hZmOBiRh6GLD6OT7ZdQF5xmdjxiEhHWFSIyGBJJBK82LYhDrzTDUPaNIAgAL+euIEe849g94UMseMRkQ6wqBCRwbO3NMX8sNZY/Xo7eDlY4LaqBBN+O403fj2FtJwiseMR0VNgUSGieqNTE0fsntYFEc/6QC6VYF/CbfScfwSRUdeg1hjsnRiIjBqLChHVK2YmMrzb2w9/TOmMtp52KChV47MdCRjyYxTi03LFjkdET4hFhYjqpWau1tg4vgO+GNQc1mZynLuZi+cXRWH2zkQUlpaLHY+IqolFhYjqLalUglHtPXHg7a7o38INao2AZUeT0es/R3HoUqbY8YioGlhUiKjec7Yxww8j2+CX0cFoYGuOm9lFGBMZg0lrziAzr1jseET0CCwqRGQ0nvNzwd63uuD1Z7whlQC/n09H92+PYP7eS8gu4ESHRPqIkxISkVG6cCsXH26OQ9ytigG2lqYyjOrgiTc6N4ajlULkdET125N8f7OoEJHR0mgE7InPwMKDSUhIVwEAzEykGBHaCOO7+MBVaSZyQqL6iUWFiOgJCIKAgxcz8f3BJJxLzQEAmMqkeCm4ISZ09YGHvYW4AYnqGRYVIqIaEAQBx5LuYuGBJERfzwIAyKUSDA5qgInPNoG3o6XICYnqBxYVIqKn9FfyPSw8mIRjSXcBAFIJMLCVOyY92wS+LtYipyMybCwqREQ6ciYlG4sOJuHgxYr7rkgkQJ9AV0x6rgkC3ZUipyMyTCwqREQ6duFWLhYdTMLu+P/NytzD3xmTnvNFaw9b8YIRGSAWFSKiWnIpIw8/HErC7+fTcH+ew86+jpj8nC9Cve3FDUdkIFhUiIhqWfKdfPx4+Cq2xN7SzszcztseU7r7oqOPAyQSicgJifQXiwoRUR1JzSrEj4evYtPpVJSpK/45DWpkiynP+aJbMycWFqIqsKgQEdWx9NwiLD2SjLXRKSgp1wAAmjewwaRnfdErwAVSKQsL0X0sKkREIsnMK8ZPf17DbydvoLBUDQBo5mKNiOeaoH8LN8hYWIhYVIiIxJZVUIpfjl3DyuPXkVdSDgBo7GiJic82wQut3WEi45ywZLxYVIiI9ERuURlWHr+On49dQ25RGQDAw94cb3ZtgqFtG0Ahl4mckKjusagQEemZ/JJyrDpxAz/9mYx7BaUAADelGcZ3aYzhoY1gZsLCQsaDRYWISE8VlaqxNjoFS49exW1VCQDA0UqBcV28MbKdJywVcpETEtU+FhUiIj1XXKbGptM3sfjwVdzKKQIA2FmY4PXOjfFKB0/YmJmInJCo9rCoEBEZiDK1BlvO3MIPh5Nw414hAMDGTI7RnbwxtpMXbC1MRU5IpHssKkREBqZcrcHv59Ox6FASkjLzAQCWpjKM6uCJNzo3hqOVQuSERLrDokJEZKA0GgG74zOw8GASEtNVAAAzEyleDvXEuC6N4ao0Ezkh0dNjUSEiMnCCIOBAYiYWHryCczdzAQCmMinCQhpiQlcfNLSzEDkhUc2xqBAR1ROCIODPK3ex8OAVxFzPBgDIpRIMadMAE7s1gZejpcgJiZ4ciwoRUT0jCAJOJmdh0aEriEq6BwCQSoDnW7lj0nNN0MTZWuSERNXHokJEVI+dvpGNRQev4NClOwAAiQToE+iKV9p7on1jB06ASHqPRYWIyAjE3czFwoNXsDfhtnaZh705wtp6YGjbhnC3NRcxHdHDsagQERmRixkq/HriBnacTdNOgCiVAJ19nTAsxAM9/F1gKuckiKQ/WFSIiIxQUakaO+PSsf5UKqKvZWmX21uaYnBQAwwL8UBTF45lIfGxqBARGblrdwuw4VQq/nv6JjLzSrTLW3vYYliIBwa2cocV5xUikbCoEBERgIo73h65fAfrY1Jx8GImyjUV/+Sbm8jQv6UbhoV4INjTDhIJB+BS3WFRISKiB9zJK8HmMzex/lQqku8UaJc3drJEWLAHhrRpAGdr3vmWah+LChERPZQgCDh9IxvrY1LxR1w6CkvVAACZVILn/JwxLNgD3Zo5QS7jAFyqHSwqRERULfkl5fj9XBrWn0pFbEqOdrmztQJD2zZEWLAHvHn3W9IxFhUiInpiV27nYX1MKjbH3kJWQal2eaiXPcJCPNCvhSssTDkAl54eiwoREdVYabkGBxJvY8OpVBy5fAd/j7+FlUKOga3cMSzEA60aKjkAl2qMRYWIiHQiPbcI/z19ExtO3URKVqF2uZ+rNV4K9sDgoAawtzQVMSEZIhYVIiLSKY1GwMlr97AhJhW7LmSgpFwDADCVSdEzwAVhIR54pokjZJxniKqBRYWIiGpNblEZtp+9hfWnUnHhlkq73F1phheDPfBS24bwsLcQMSHpOxYVIiKqE/FpudgQk4qtZ9OQW1QGoGI2504+jggL8UCvABeYmchETkn6hkWFiIjqVHGZGnviM7DhVCqiku5plyvNTTA4qAHCgj0Q4M5/p6kCiwoREYkmNasQG0/fxKZTqUjLLdYub9FAibAQDzzfyh1KcxMRE5LYWFSIiEh0ao2AY0l3sSEmFXsTMlCmrvi6Ucil6NfCDWHBHmjf2J6XORshFhUiItIrWQWl2BJ7CxtiUnHpdp52uaeDBcKCPTC0TUO4KjnPkLFgUSEiIr0kCALO3czF+phU7DiXhvyScgCAVAJ0a+aMsOCG6NbMmQNw6zkWFSIi0nuFpeXYGZeBDTGpiL6epV1ubiJD16ZO6BXoguf8nGFrwRvK1TcsKkREZFCS7+Rj4+mb2BZ7q9IAXJlUglAve/QKdEHPABc0tOP9WeoDFhUiIjJIgiAgPk2FvQm3sTc+Axcz8iqtD3Cz0ZaWADcbDsQ1UAZTVObMmYPNmzfj4sWLMDc3R8eOHTF37lw0a9asWvuzqBAR1W+pWYXa0hJzPUs7QSIANLA1R69AF/QKcEWIlx3kMql4QemJGExR6dOnD4YPH46QkBCUl5fjo48+woULF5CQkABLS8vH7s+iQkRkPLIKSnHwYib2xmfg6JU7KC7TaNfZWpjgOT9n9ApwQZemTrAwlYuYlB7HYIrKv925cwfOzs44cuQIunTp8tjtWVSIiIxTUakax5LuYm98BvYn3kZ2YZl2nUIuRWdfR/QMcEF3fxc4WilETEpVeZLvb72qnLm5uQAAe3v7KteXlJSgpKRE+1ylUlW5HRER1W/mpjL0DKgYq1Ku1uD0jWzsS7iNvQm3kZJViP2JmdifmAmJJA7BnnboGVBxisjL8fFH60m/6M0RFY1Gg+effx45OTk4duxYldvMnDkTn3322QPLeUSFiIiAisG4l2/nY298BvYm3EbcrdxK65u6WGlLS4sGSkilHIwrBoM89fPmm29i165dOHbsGBo2bFjlNlUdUfHw8GBRISKiKqXlFGF/4m3sjb+Nk8n3UP6P0biuNmboEeCMXgGuaN/YAaZyDsatKwZXVCZNmoRt27bh6NGj8Pb2rvZ+HKNCRETVlVtYhkOXMrEv4TYOX8pEQalau85aIcezfs7oGeCCbs2cYG3GSRNrk8EUFUEQMHnyZGzZsgWHDx+Gr6/vE+3PokJERDVRXKbGieR72Bt/G/sSbuNu/v+O1pvIJOjo46gdA+NiwzmIdM1gisrEiROxZs0abNu2rdK9U5RKJczNzR+7P4sKERE9LY1GQGxqTsVg3PgMJN8tqLS+tYctega4oHegC3ycrHiTOR0wmKLysF92ZGQkRo8e/dj9WVSIiEjXkjLz/76CKAOxKTmV1jV2tKwYjBvogiAPOw7GrSGDKSpPi0WFiIhqU6aqGPsTM7E3IQPHk+6hVP2/m8w5WinQw98ZvQJd0NHHkTM+PwEWFSIiIh3LKy7Dkct3sC/hNg5ezERecbl2nYXpP2Z8buYCpQUH4z4KiwoREVEtKi3X4K9r/xuMm6GqPONzO297POPriPaNHdCigRImnIeoEhYVIiKiOiIIAuJu5WpLy6XblWd8tjSVIdjLHu0bO6B9Y3u0aKA0+gkUWVSIiIhEcuNeAQ5ezMSJq/fw17Us5BaVVVpvaSpDiPf94uKA5u42RldcWFSIiIj0gEYj4GJGHk4m38PJ5KqLi5VCjmAvO3T4u7gEGkFxYVEhIiLSQxqNgMQMFU4mZ1UUl+R7UP1jUC5QUVxCvOzQvrEDOvg4IMCt/hUXFhUiIiIDoNYIuJihwomr93AyOQvR1x4sLtYK+d+niuz/PuKihMzA79/CokJERGSA1BoBiemqSqeK8qooLqH/GOMS4G5jcMWFRYWIiKge+GdxOXH1HqKvZSGv5F/FxUyOUC97dPCpKC7+bvpfXFhUiIiI6iG1RkBC2v+OuDysuLT7xxEXfSwuLCpERERGoFytQYL2VFEWoq9lIf9fxcXGTI5QbwftGBd9KC4sKkREREbofnGpGJx7DzHXsx8oLkpzk3+McbGHv6tNnU+uyKJCREREKFdrEP/3qaITyfcQcy0LBaXqStvcLy737+Pi52pd68WFRYWIiIgeUK7W4ELa/wbnnrpedXG5P8alg48DmrnovriwqBAREdFjlak1uHArV3sDupjrWSj8V3F5pokjfnu9nU7f90m+v+U6fWciIiIyGCYyKYIa2SGokR3e7OaDMrUGcbdytYNzT13PQmADcQ8EsKgQERERgIri0qaRHdo0ssPEbhVHXIrK1I/drzaxqBAREVGVTGRSmIg8z1D9muWIiIiI6hUWFSIiItJbLCpERESkt1hUiIiISG+xqBAREZHeYlEhIiIivcWiQkRERHqLRYWIiIj0FosKERER6S0WFSIiItJbLCpERESkt1hUiIiISG+xqBAREZHeMujZkwVBAACoVCqRkxAREVF13f/evv89/igGXVTy8vIAAB4eHiInISIioieVl5cHpVL5yG0kQnXqjJ7SaDRIS0uDtbU1JBKJTl9bpVLBw8MDqampsLGx0elr05Pj70O/8PehX/j70D/8nTyaIAjIy8uDu7s7pNJHj0Ix6CMqUqkUDRs2rNX3sLGx4V8yPcLfh37h70O/8Pehf/g7ebjHHUm5j4NpiYiISG+xqBAREZHeYlF5CIVCgU8//RQKhULsKAT+PvQNfx/6hb8P/cPfie4Y9GBaIiIiqt94RIWIiIj0FosKERER6S0WFSIiItJbLCpERESkt1hUqvDDDz/Ay8sLZmZmaNeuHaKjo8WOZLTmzJmDkJAQWFtbw9nZGYMGDcKlS5fEjkUAvvrqK0gkEkybNk3sKEbt1q1bGDVqFBwcHGBubo4WLVrg1KlTYscySmq1GjNmzIC3tzfMzc3h4+ODzz//vFrz2dDDsaj8y/r16/H222/j008/xZkzZ9CqVSv07t0bmZmZYkczSkeOHEFERAROnjyJffv2oaysDL169UJBQYHY0YxaTEwMli5dipYtW4odxahlZ2ejU6dOMDExwa5du5CQkIB58+bBzs5O7GhGae7cuVi8eDEWLVqExMREzJ07F19//TUWLlwodjSDxsuT/6Vdu3YICQnBokWLAFTMJ+Th4YHJkyfjgw8+EDkd3blzB87Ozjhy5Ai6dOkidhyjlJ+fjzZt2uDHH3/EF198gdatW2PBggVixzJKH3zwAaKiovDnn3+KHYUADBgwAC4uLvj555+1y4YOHQpzc3P89ttvIiYzbDyi8g+lpaU4ffo0evTooV0mlUrRo0cPnDhxQsRkdF9ubi4AwN7eXuQkxisiIgL9+/ev9N8JiWP79u0IDg7GSy+9BGdnZwQFBWH58uVixzJaHTt2xIEDB3D58mUAwLlz53Ds2DH07dtX5GSGzaAnJdS1u3fvQq1Ww8XFpdJyFxcXXLx4UaRUdJ9Go8G0adPQqVMnNG/eXOw4RmndunU4c+YMYmJixI5CAJKTk7F48WK8/fbb+OijjxATE4MpU6bA1NQU4eHhYsczOh988AFUKhX8/Pwgk8mgVqvx5ZdfYuTIkWJHM2gsKmQwIiIicOHCBRw7dkzsKEYpNTUVU6dOxb59+2BmZiZ2HEJFeQ8ODsbs2bMBAEFBQbhw4QKWLFnCoiKCDRs2YPXq1VizZg0CAwNx9uxZTJs2De7u7vx9PAUWlX9wdHSETCbD7du3Ky2/ffs2XF1dRUpFADBp0iT8/vvvOHr0KBo2bCh2HKN0+vRpZGZmok2bNtplarUaR48exaJFi1BSUgKZTCZiQuPj5uaGgICASsv8/f3x3//+V6RExu3dd9/FBx98gOHDhwMAWrRogRs3bmDOnDksKk+BY1T+wdTUFG3btsWBAwe0yzQaDQ4cOIAOHTqImMx4CYKASZMmYcuWLTh48CC8vb3FjmS0unfvjri4OJw9e1b7CA4OxsiRI3H27FmWFBF06tTpgcv1L1++DE9PT5ESGbfCwkJIpZW/VmUyGTQajUiJ6gceUfmXt99+G+Hh4QgODkZoaCgWLFiAgoICjBkzRuxoRikiIgJr1qzBtm3bYG1tjYyMDACAUqmEubm5yOmMi7W19QNjgywtLeHg4MAxQyJ566230LFjR8yePRthYWGIjo7GsmXLsGzZMrGjGaWBAwfiyy+/RKNGjRAYGIjY2FjMnz8fY8eOFTuaYRPoAQsXLhQaNWokmJqaCqGhocLJkyfFjmS0AFT5iIyMFDsaCYLQtWtXYerUqWLHMGo7duwQmjdvLigUCsHPz09YtmyZ2JGMlkqlEqZOnSo0atRIMDMzExo3bix8/PHHQklJidjRDBrvo0JERER6i2NUiIiISG+xqBAREZHeYlEhIiIivcWiQkRERHqLRYWIiIj0FosKERER6S0WFSIiItJbLCpEVK9IJBJs3bpV7BhEpCMsKkSkM6NHj4ZEInng0adPH7GjEZGB4lw/RKRTffr0QWRkZKVlCoVCpDREZOh4RIWIdEqhUMDV1bXSw87ODkDFaZnFixejb9++MDc3R+PGjbFp06ZK+8fFxeG5556Dubk5HBwcMG7cOOTn51fa5pdffkFgYCAUCgXc3NwwadKkSuvv3r2LwYMHw8LCAr6+vti+fXvtfmgiqjUsKkRUp2bMmIGhQ4fi3LlzGDlyJIYPH47ExEQAQEFBAXr37g07OzvExMRg48aN2L9/f6UisnjxYkRERGDcuHGIi4vD9u3b0aRJk0rv8dlnnyEsLAznz59Hv379MHLkSGRlZdXp5yQiHRF7VkQiqj/Cw8MFmUwmWFpaVnp8+eWXgiBUzIY9YcKESvu0a9dOePPNNwVBEIRly5YJdnZ2Qn5+vnb9H3/8IUilUiEjI0MQBEFwd3cXPv7444dmACD83//9n/Z5fn6+AEDYtWuXzj4nEdUdjlEhIp169tlnsXjx4krL7O3ttX/u0KFDpXUdOnTA2bNnAQCJiYlo1aoVLC0ttes7deoEjUaDS5cuQSKRIC0tDd27d39khpYtW2r/bGlpCRsbG2RmZtb0IxGRiFhUiEinLC0tHzgVoyvm5ubV2s7ExKTSc4lEAo1GUxuRiKiWcYwKEdWpkydPPvDc398fAODv749z586hoKBAuz4qKgpSqRTNmjWDtbU1vLy8cODAgTrNTETi4REVItKpkpISZGRkVFoml8vh6OgIANi4cSOCg4PxzDPPYPXq1YiOjsbPP/8MABg5ciQ+/fRThIeHY+bMmbhz5w4mT56MV155BS4uLgCAmTNnYsKECXB2dkbfvn2Rl5eHqKgoTJ48uW4/KBHVCRYVItKp3bt3w83NrdKyZs2a4eLFiwAqrshZt24dJk6cCDc3N6xduxYBAQEAAAsLC+zZswdTp05FSEgILCwsMHToUMyfP1/7WuHh4SguLsZ//vMfTJ8+HY6OjnjxxRfr7gMSUZ2SCIIgiB2CiIyDRCLBli1bMGjQILGjEJGB4BgVIiIi0lssKkRERKS3OEaFiOoMzzQT0ZPiERUiIiLSWywqREREpLdYVIiIiEhvsagQERGR3mJRISIiIr3FokJERER6i0WFiIiI9BaLChEREektFhUiIiLSW/8PMT07rBu1M8AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset from the given URL\n",
    "def load_dataset(url):\n",
    "    df = pd.read_csv(url, header=None, names=[\"Joke\"])\n",
    "    return df\n",
    "\n",
    "# Preprocess dataset\n",
    "def preprocess_data(df):\n",
    "    jokes = df['Joke'].str.lower().str.replace('[^a-z\\s]', '', regex=True)\n",
    "    all_words = ' '.join(jokes).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "    return jokes, vocab\n",
    "\n",
    "# Custom Dataset class\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes, vocab, sequence_length):\n",
    "        self.vocab = vocab\n",
    "        self.sequence_length = sequence_length\n",
    "        self.jokes = [\n",
    "            [vocab[word] for word in joke.split() if word in vocab]\n",
    "            for joke in jokes\n",
    "        ]\n",
    "        self.data = []\n",
    "        for joke in self.jokes:\n",
    "            for i in range(len(joke) - sequence_length):\n",
    "                self.data.append((joke[i:i + sequence_length], joke[i + 1:i + 1 + sequence_length]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Training function\n",
    "def train_lstm(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(x)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        loss_history.append(epoch_loss / len(dataloader))\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "    return loss_history\n",
    "\n",
    "# Prediction functions\n",
    "def predict_lstm_argmax(model, input_seq):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(input_seq.unsqueeze(0))\n",
    "        return torch.argmax(output, dim=-1).squeeze().tolist()\n",
    "\n",
    "def predict_lstm_softmax(model, input_seq):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(input_seq.unsqueeze(0))\n",
    "        probabilities = torch.softmax(output, dim=-1)\n",
    "        return torch.multinomial(probabilities.squeeze(), 1).tolist()\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "url = \"https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\"\n",
    "dataset = load_dataset(url)\n",
    "jokes, vocab = preprocess_data(dataset)\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 5\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare data for training\n",
    "train_jokes, val_jokes = train_test_split(jokes, test_size=0.2, random_state=42)\n",
    "train_dataset = JokesDataset(train_jokes, vocab, sequence_length)\n",
    "val_dataset = JokesDataset(val_jokes, vocab, sequence_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "vocab_size = len(vocab) + 1  # +1 for padding index\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "loss_history = train_lstm(model, train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(loss_history, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5853e1d6-f09a-4b42-bbff-b09ce034d7c1",
   "metadata": {},
   "source": [
    "1. The LSTM model effectively captures sequential dependencies in the jokes dataset due to its ability to maintain long-term context. \n",
    "2. However, the dataset's limited size and the lack of labels for supervised tasks restrict the performance evaluation. \n",
    "3. While LSTM provides richer representations compared to simpler models like logistic regression, it is computationally more expensive.\n",
    "\n",
    "\n",
    "Architecture:\n",
    "\n",
    "RNN (Recurrent Neural Network): RNNs process sequential data by passing information through loops in the network. At each timestep, the output of the previous timestep is fed back into the network as input for the next timestep. However, RNNs struggle to maintain long-term dependencies because of the vanishing gradient problem.\n",
    "\n",
    "LSTM (Long Short-Term Memory): LSTM is a special type of RNN designed to better handle long-term dependencies. It uses gates (input, forget, and output gates) to regulate the flow of information and prevent vanishing gradients. These gates allow the model to \"remember\" information for longer periods.\n",
    "\n",
    "Performance:\n",
    "RNN: Performs well for short sequences where long-term dependencies aren't critical. However, it struggles with longer sequences due to the vanishing gradient problem, making it less effective for tasks like language modeling or machine translation.\n",
    "\n",
    "LSTM: Handles long-term dependencies much better than traditional RNNs. LSTMs perform better in tasks such as speech recognition, machine translation, and time-series forecasting, where long-range context is crucial.\n",
    "\n",
    "Which Model Performs Better?\n",
    "LSTM generally performs better than RNN in tasks with long-range dependencies or more complex sequence patterns. The gating mechanism in LSTMs allows them to remember important information and avoid the vanishing gradient problem that plagues standard RNNs.\n",
    "Causes for Performance Differences:\n",
    "Memory Retention: LSTMs are better at retaining important information across long sequences due to their gating mechanisms, while RNNs tend to forget earlier parts of the sequence.\n",
    "Gradient Problems: RNNs face vanishing and exploding gradients during training, which makes them less effective for long sequences. LSTMs mitigate this with their internal memory structure and better gradient flow.\n",
    "Limitations:\n",
    "RNN:\n",
    "Struggles with long-term dependencies due to the vanishing gradient problem.\n",
    "Slower to train on large datasets because it has fewer parameters than more complex models like LSTMs.\n",
    "LSTM:\n",
    "More computationally expensive than RNN due to the additional gates and more complex architecture.\n",
    "Still not perfect in handling extremely long sequences or very complex dependencies without further modifications (e.g., attention mechanisms).\n",
    "May overfit on small datasets due to the larger number of parameters compared to a simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db2146",
   "metadata": {
    "id": "54db2146"
   },
   "source": [
    "### Task 2 (2 points)\n",
    "\n",
    "The goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\n",
    "\n",
    "a) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\n",
    "\n",
    "b) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n",
    "\n",
    "**Hint:** If you don't have access to GPU, you can downsample the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287d740",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8287d740",
    "outputId": "b4751b95-a368-40da-e979-b2c7abfdaa0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "{'text': '::but I like baby bangs:: /tiny voice', 'labels': [18], 'id': 'ed5h3jh'}\n"
     ]
    }
   ],
   "source": [
    "# Here comes your code\n",
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "validation_set = dataset['validation']\n",
    "\n",
    "print(validation_set[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f787dbb",
   "metadata": {
    "id": "3f787dbb"
   },
   "source": [
    "### Task 3 (3 points)\n",
    "\n",
    "a) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n",
    "2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n",
    "3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n",
    "4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\n",
    "\n",
    "b) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n",
    "\n",
    "**Hint:** The output should be as follows:\n",
    "\n",
    "`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7431dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f7431dc",
    "outputId": "3f0ddffe-ef70-4c43-ce5e-b2f5573af2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_data(example):\n",
    "    # Tokenize the text\n",
    "    encoding = tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    # Convert labels to a multi-hot format\n",
    "    labels = torch.zeros(len(dataset['train'].features['labels'].feature.names))\n",
    "    labels[example['labels']] = 1\n",
    "    encoding['labels'] = labels\n",
    "    return encoding\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "encoded_dataset = dataset.map(tokenize_data, batched=False)\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "if 'validation' in encoded_dataset:\n",
    "    validation_set = encoded_dataset['validation']\n",
    "    print(validation_set[-1].keys())\n",
    "else:\n",
    "    print(\"The dataset does not have a validation split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hafPL0Yh6S8W",
   "metadata": {
    "id": "hafPL0Yh6S8W"
   },
   "source": [
    "### Task 4 (15 points)\n",
    "\n",
    "Implement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\n",
    "\n",
    "a) **BERT Approach:**\n",
    "\n",
    "\n",
    "``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "       \n",
    "- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n",
    "        \n",
    "- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n",
    "        \n",
    "``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n",
    "\n",
    "**Hints:**\n",
    "- Utilize `TrainingArguments` and `Trainer` classes.\n",
    "\n",
    "- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\n",
    "\n",
    "b) **Alternative Approach:**\n",
    "\n",
    "\n",
    "``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "  - Use scikit-learn library for the  implementations.\n",
    "\n",
    "``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n",
    "\n",
    "__Hints:__\n",
    "\n",
    "  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n",
    "  \n",
    "``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\n",
    "\n",
    "c) **Discussion:**\n",
    "\n",
    " Discuss the strengths and weaknesses of each approach.\n",
    "\n",
    "__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Lo2B4UOBpV-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "1de91fd311b4476c92570b736c575a93",
      "3eb139f07ab44a1c813f99ee0d399c5a",
      "9ff9e65fd8924f8fb807bddd5baffd4d",
      "b24c27b92547409db2e4ba94d23560f5",
      "2c6c2eb3a05646edbff8e771c51d1ef6",
      "9bf1d38ee0ba44b9811a5972a1a5ac41",
      "b378e7426f264754bd375c64cdd826e8",
      "6cfa4c6e4c314662a2248054cc6a7ca6",
      "16e8e19821294f69ae229856b2f4ce24",
      "695ba4f4008c492181df312143614833",
      "84b8fe813a3443c0ba3b926c804d1740",
      "da9137e428674e69968eeb4657b25537",
      "b83676813fba4b3897605d21c8d6c9b9",
      "7cf96cf9a5ed4e339495a22e7786d799",
      "33c7ef3096294deea6f3b3d7b7d55476",
      "cf3d556684d34b07ad407daa2f48de8e",
      "4e9794a03c8a4d7caeb1feec1b26c378",
      "70f2228e00ff4b378b2f21366a71d367",
      "4ab96e51d21546839d689844a0d7f724",
      "f6631fdc06e543c59eee2b0b3f48ba89",
      "19aa1034e7294248bdcbac7a4e300863",
      "ef0b924952c6481f8afda41e0313eb9f"
     ]
    },
    "id": "-Lo2B4UOBpV-",
    "outputId": "3bdf3e5b-ace3-4e39-b2ff-e78e1af8ae88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de91fd311b4476c92570b736c575a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9137e428674e69968eeb4657b25537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 08:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335753</td>\n",
       "      <td>0.233634</td>\n",
       "      <td>0.655254</td>\n",
       "      <td>0.760391</td>\n",
       "      <td>0.575659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.307761</td>\n",
       "      <td>0.272009</td>\n",
       "      <td>0.685248</td>\n",
       "      <td>0.793544</td>\n",
       "      <td>0.602962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.305541</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>0.693498</td>\n",
       "      <td>0.783673</td>\n",
       "      <td>0.621934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Results: {'eval_loss': 0.3055413067340851, 'eval_accuracy': 0.2866817155756208, 'eval_f1': 0.6934984520123839, 'eval_precision': 0.7836734693877551, 'eval_recall': 0.6219342896807034, 'eval_runtime': 6.0108, 'eval_samples_per_second': 147.402, 'eval_steps_per_second': 9.317, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#Load and Preprocess Dataset\n",
    "dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "train_texts = [x[\"Tweet\"] for x in dataset[\"train\"]]\n",
    "val_texts = [x[\"Tweet\"] for x in dataset[\"validation\"]]\n",
    "\n",
    "labels = [\n",
    "    \"anger\", \"anticipation\", \"disgust\", \"fear\",\n",
    "    \"joy\", \"love\", \"optimism\", \"pessimism\",\n",
    "    \"sadness\", \"surprise\", \"trust\"\n",
    "]\n",
    "\n",
    "train_labels = np.array([[int(x[label]) for label in labels] for x in dataset[\"train\"]], dtype=np.float32)\n",
    "val_labels = np.array([[int(x[label]) for label in labels] for x in dataset[\"validation\"]], dtype=np.float32)\n",
    "\n",
    "#Tokenize Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels}).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"labels\": val_labels}).map(tokenize_function, batched=True)\n",
    "\n",
    "#Convert labels to float tensors for compatibility with the loss function\n",
    "def format_dataset(dataset):\n",
    "    dataset = dataset.remove_columns([\"text\"])  # Remove the text column\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return dataset\n",
    "\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "val_dataset = format_dataset(val_dataset)\n",
    "\n",
    "#Define the Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "#Define Training Arguments and Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    # labels is already a numpy array; no need for .numpy()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"micro\"),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"micro\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"micro\"),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#Train the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"BERT Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IhqwKwMe6S8X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IhqwKwMe6S8X",
    "outputId": "ff45b66e-d23b-42ab-fd20-b682f6554dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Logistic Regression Results: {'accuracy': 0.9841986455981941, 'f1': 0.9960339943342776, 'precision': 0.9920993227990971, 'recall': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/multiclass.py:87: UserWarning: Label 0 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and Preprocess Dataset\n",
    "dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2018_task_1\", \"subtask5.english\")\n",
    "train_texts = [x[\"Tweet\"] for x in dataset[\"train\"]]\n",
    "val_texts = [x[\"Tweet\"] for x in dataset[\"validation\"]]\n",
    "\n",
    "labels = [\n",
    "    \"anger\", \"anticipation\", \"disgust\", \"fear\",\n",
    "    \"joy\", \"love\", \"optimism\", \"pessimism\",\n",
    "    \"sadness\", \"surprise\", \"trust\"\n",
    "]\n",
    "\n",
    "train_labels = [[int(x[label]) for label in labels] for x in dataset[\"train\"]]\n",
    "val_labels = [[int(x[label]) for label in labels] for x in dataset[\"validation\"]]\n",
    "\n",
    "# Step 2: Vectorize Text Using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_val = vectorizer.transform(val_texts)\n",
    "\n",
    "# Step 3: Binarize Multilabel Targets\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_labels)\n",
    "y_val = mlb.transform(val_labels)\n",
    "\n",
    "# Step 4: Train Logistic Regression Model\n",
    "classifier = OneVsRestClassifier(LogisticRegression(solver=\"liblinear\"))\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict on Validation Data\n",
    "y_pred = classifier.predict(X_val)\n",
    "\n",
    "# Step 6: Evaluate Model\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_val, y_pred),\n",
    "    \"f1\": f1_score(y_val, y_pred, average=\"micro\"),\n",
    "    \"precision\": precision_score(y_val, y_pred, average=\"micro\"),\n",
    "    \"recall\": recall_score(y_val, y_pred, average=\"micro\"),\n",
    "}\n",
    "\n",
    "print(\"TF-IDF + Logistic Regression Results:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2yaPfg6n6S8X",
   "metadata": {
    "id": "2yaPfg6n6S8X"
   },
   "source": [
    "#### Here comes your discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bYoX6YsG8jP",
   "metadata": {
    "id": "8bYoX6YsG8jP"
   },
   "outputs": [],
   "source": [
    "Discussion: Strengths and Weaknesses of Each Approach\n",
    "1. BERT-Based Approach\n",
    "Strengths:\n",
    "\n",
    "State-of-the-Art Performance: BERT leverages deep bidirectional transformers, capturing context in both directions, which makes it highly effective for complex natural language understanding tasks.\n",
    "Multilabel Classification Capability: With the proper configuration (problem_type=\"multi_label_classification\"), BERT handles multilabel outputs seamlessly, reducing the need for additional feature engineering.\n",
    "Pre-trained Knowledge: BERT models are pre-trained on large corpora, enabling them to generalize well, even on smaller datasets.\n",
    "Flexibility: The HuggingFace library offers robust tools for fine-tuning, metric computation, and model customization.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally Expensive: BERT requires significant computational resources, especially during training, due to its large model size and transformer architecture.\n",
    "Longer Training Time: Compared to traditional methods, fine-tuning BERT can be time-intensive, particularly on large datasets.\n",
    "Hyperparameter Sensitivity: The performance of BERT is sensitive to hyperparameters like learning rate, batch size, and the number of epochs, requiring extensive experimentation for optimal results.\n",
    "Improvements:\n",
    "\n",
    "Experimenting with learning rate schedulers and optimizers to enhance convergence.\n",
    "Applying data augmentation techniques to improve generalization on small datasets.\n",
    "Using variants of BERT, such as DistilBERT or RoBERTa, to reduce computational overhead.\n",
    "2. Alternative Approach (e.g., TF-IDF + SVM or Bag of Words + Logistic Regression)\n",
    "Strengths:\n",
    "\n",
    "Simplicity: Traditional models like TF-IDF combined with SVM or Logistic Regression are straightforward to implement and interpret.\n",
    "Efficiency: These methods are computationally less intensive, making them suitable for scenarios with limited resources.\n",
    "Baseline Performance: Alternative approaches often provide reliable baseline performance, which can guide the development of more complex models.\n",
    "Explainability: Models like Logistic Regression allow for easier interpretation of feature importance compared to neural networks.\n",
    "Weaknesses:\n",
    "\n",
    "Limited Contextual Understanding: Techniques like TF-IDF and Bag of Words do not capture word order or context, leading to suboptimal results for tasks requiring semantic understanding.\n",
    "Feature Engineering: These approaches often require manual feature engineering, which may not be feasible for large or complex datasets.\n",
    "Scalability Issues: With a high-dimensional feature space (e.g., large vocabularies), traditional methods may suffer from scalability challenges.\n",
    "Improvements:\n",
    "\n",
    "Using dimensionality reduction techniques (e.g., PCA) to manage high-dimensional data.\n",
    "Enhancing tokenization with n-grams or custom preprocessing to capture more semantic information.\n",
    "Fine-tuning hyperparameters like the regularization term in SVM or Logistic Regression to boost performance.\n",
    "Comparative Analysis\n",
    "Performance: BERT generally outperforms traditional methods on complex multilabel classification tasks, given its ability to capture nuanced language patterns. However, the gap narrows on simpler datasets or tasks with limited contextual dependencies.\n",
    "Resources: Traditional methods excel in resource-constrained environments, while BERT's requirements for memory and compute power are higher.\n",
    "Ease of Use: While BERT demands familiarity with deep learning frameworks, traditional approaches can be quickly implemented with libraries like scikit-learn.\n",
    "In summary, the choice between BERT and traditional methods depends on the task's complexity, the dataset size, and the computational resources available. Exploring variations, such as fine-tuning hyperparameters or testing hybrid approaches (e.g., combining TF-IDF features with a neural network), can further enhance performance and efficiency."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16e8e19821294f69ae229856b2f4ce24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "19aa1034e7294248bdcbac7a4e300863": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1de91fd311b4476c92570b736c575a93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3eb139f07ab44a1c813f99ee0d399c5a",
       "IPY_MODEL_9ff9e65fd8924f8fb807bddd5baffd4d",
       "IPY_MODEL_b24c27b92547409db2e4ba94d23560f5"
      ],
      "layout": "IPY_MODEL_2c6c2eb3a05646edbff8e771c51d1ef6"
     }
    },
    "2c6c2eb3a05646edbff8e771c51d1ef6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33c7ef3096294deea6f3b3d7b7d55476": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19aa1034e7294248bdcbac7a4e300863",
      "placeholder": "",
      "style": "IPY_MODEL_ef0b924952c6481f8afda41e0313eb9f",
      "value": "886/886[00:00&lt;00:00,5884.43examples/s]"
     }
    },
    "3eb139f07ab44a1c813f99ee0d399c5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bf1d38ee0ba44b9811a5972a1a5ac41",
      "placeholder": "",
      "style": "IPY_MODEL_b378e7426f264754bd375c64cdd826e8",
      "value": "Map:100%"
     }
    },
    "4ab96e51d21546839d689844a0d7f724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e9794a03c8a4d7caeb1feec1b26c378": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695ba4f4008c492181df312143614833": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cfa4c6e4c314662a2248054cc6a7ca6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f2228e00ff4b378b2f21366a71d367": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cf96cf9a5ed4e339495a22e7786d799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ab96e51d21546839d689844a0d7f724",
      "max": 886,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6631fdc06e543c59eee2b0b3f48ba89",
      "value": 886
     }
    },
    "84b8fe813a3443c0ba3b926c804d1740": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bf1d38ee0ba44b9811a5972a1a5ac41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ff9e65fd8924f8fb807bddd5baffd4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cfa4c6e4c314662a2248054cc6a7ca6",
      "max": 6838,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16e8e19821294f69ae229856b2f4ce24",
      "value": 6838
     }
    },
    "b24c27b92547409db2e4ba94d23560f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_695ba4f4008c492181df312143614833",
      "placeholder": "",
      "style": "IPY_MODEL_84b8fe813a3443c0ba3b926c804d1740",
      "value": "6838/6838[00:01&lt;00:00,6632.63examples/s]"
     }
    },
    "b378e7426f264754bd375c64cdd826e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b83676813fba4b3897605d21c8d6c9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e9794a03c8a4d7caeb1feec1b26c378",
      "placeholder": "",
      "style": "IPY_MODEL_70f2228e00ff4b378b2f21366a71d367",
      "value": "Map:100%"
     }
    },
    "cf3d556684d34b07ad407daa2f48de8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da9137e428674e69968eeb4657b25537": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b83676813fba4b3897605d21c8d6c9b9",
       "IPY_MODEL_7cf96cf9a5ed4e339495a22e7786d799",
       "IPY_MODEL_33c7ef3096294deea6f3b3d7b7d55476"
      ],
      "layout": "IPY_MODEL_cf3d556684d34b07ad407daa2f48de8e"
     }
    },
    "ef0b924952c6481f8afda41e0313eb9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6631fdc06e543c59eee2b0b3f48ba89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
